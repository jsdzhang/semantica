{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/trading/01_Risk_Assessment.ipynb)\n",
        "\n",
        "# Risk Assessment - Graph Analytics & Portfolio Risk Modeling\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **portfolio risk assessment** using Semantica with focus on **graph-based analytics**, **portfolio risk modeling**, **market simulation**, and **dependency analysis**. The pipeline assesses portfolio risk using graph-based analytics and market simulations.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Graph-Based Analytics**: Uses graph analytics for portfolio risk analysis\n",
        "- **Portfolio Risk Modeling**: Models portfolio relationships and dependencies\n",
        "- **Market Simulation**: Simulates market scenarios using graph data\n",
        "- **Dependency Analysis**: Analyzes dependencies between portfolio components\n",
        "- **Risk Modeling**: Emphasizes graph analytics, reasoning, and risk modeling\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Portfolio Data Ingestion\n",
        "3. **Phase 2**: Entity Extraction (Price, Signal, Pattern, Indicator, Strategy)\n",
        "4. **Phase 3**: Financial Knowledge Graph Construction\n",
        "5. **Phase 4**: Graph Analytics (Dependencies, Relationships)\n",
        "6. **Phase 5**: Portfolio Risk Modeling\n",
        "7. **Phase 6**: Market Simulation\n",
        "8. **Phase 7**: Visualization & Risk Reporting\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.kg import GraphAnalytics\n",
        "from semantica.reasoning import GraphReasoner\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"Risk_Assessment\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\"}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "print(\"Configured for risk assessment with graph analytics focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Real Data Ingestion (Yahoo Finance API & RSS Feeds)\n",
        "\n",
        "Ingest portfolio data from Yahoo Finance API and financial RSS feeds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import WebIngestor, FeedIngestor, FileIngestor\n",
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.deduplication import DuplicateDetector\n",
        "from semantica.split import TextSplitter\n",
        "from semantica.context import AgentContext\n",
        "from semantica.vector_store import VectorStore\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Option 1: Ingest from Yahoo Finance API\n",
        "yahoo_api = \"https://query1.finance.yahoo.com/v8/finance/chart/AAPL\"\n",
        "try:\n",
        "    web_ingestor = WebIngestor()\n",
        "    api_documents = web_ingestor.ingest(yahoo_api, method=\"url\")\n",
        "    documents.extend(api_documents)\n",
        "except Exception as e:\n",
        "    print(f\"API ingestion failed: {e}\")\n",
        "\n",
        "# Option 2: Ingest from financial RSS feeds\n",
        "financial_feeds = [\n",
        "    # Add financial news RSS feeds here\n",
        "]\n",
        "\n",
        "for feed_url in financial_feeds:\n",
        "    try:\n",
        "        feed_ingestor = FeedIngestor()\n",
        "        feed_documents = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
        "        documents.extend(feed_documents)\n",
        "    except Exception as e:\n",
        "        print(f\"Feed ingestion failed: {e}\")\n",
        "\n",
        "# Fallback: Sample data\n",
        "if not documents:\n",
        "    portfolio_data = \"\"\"\n",
        "    Portfolio contains: Stock AAPL (30%), Stock MSFT (25%), Stock GOOGL (20%), Bond BND (25%).\n",
        "    AAPL price correlates with tech sector performance.\n",
        "    MSFT depends on cloud services market growth.\n",
        "    Portfolio risk: High concentration in tech sector (75%).\n",
        "    Dependency: Tech sector downturn impacts 75% of portfolio.\n",
        "    \"\"\"\n",
        "    with open(\"data/portfolio.txt\", \"w\") as f:\n",
        "        f.write(portfolio_data)\n",
        "    documents = FileIngestor().ingest(\"data/portfolio.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n",
        "\n",
        "# Normalize financial data\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        normalize_numbers=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Use entity-aware or graph-based chunking for portfolio relationships\n",
        "splitter = TextSplitter(method=\"entity_aware\", ner_method=\"llm\", chunk_size=1000, chunk_overlap=200)\n",
        "# Alternative: graph_based chunking\n",
        "# splitter = TextSplitter(method=\"graph_based\", knowledge_graph=None, chunk_size=1000)\n",
        "\n",
        "chunked_docs = []\n",
        "for doc_text in normalized_documents:\n",
        "    chunks = splitter.split(doc_text)\n",
        "    chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
        "\n",
        "print(f\"Created {len(chunked_docs)} entity-aware chunks\")\n",
        "\n",
        "# Build financial knowledge graph\n",
        "result = core.build_knowledge_base(\n",
        "    sources=chunked_docs,\n",
        "    custom_entity_types=[\"Price\", \"Signal\", \"Pattern\", \"Indicator\", \"Strategy\"],\n",
        "    graph=True,\n",
        "    embeddings=True\n",
        ")\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "entities = result[\"entities\"]\n",
        "\n",
        "# Deduplicate portfolio entities\n",
        "portfolio_entities = [e for e in entities if e.get(\"type\") in [\"Price\", \"Signal\", \"Pattern\"]]\n",
        "detector = DuplicateDetector()\n",
        "duplicates = detector.detect_duplicates(portfolio_entities, threshold=0.9)\n",
        "deduplicated = detector.resolve_duplicates(portfolio_entities, duplicates)\n",
        "\n",
        "print(f\"Built portfolio KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"Deduplicated: {len(portfolio_entities)} -> {len(deduplicated)} unique entities\")\n",
        "\n",
        "# Setup GraphRAG for portfolio analysis\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=384)\n",
        "if result.get(\"embeddings\"):\n",
        "    vector_store.store_vectors(\n",
        "        vectors=result[\"embeddings\"][\"vectors\"],\n",
        "        metadata=result[\"embeddings\"][\"metadata\"]\n",
        "    )\n",
        "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
        "\n",
        "print(\"Focus: Graph analytics, GraphRAG, portfolio risk modeling, market simulation, dependency analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform graph analytics for risk assessment\n",
        "analytics = GraphAnalytics(kg)\n",
        "centrality = analytics.calculate_centrality(method=\"betweenness\")\n",
        "\n",
        "# Use reasoning for dependency analysis\n",
        "reasoner = GraphReasoner(kg)\n",
        "dependencies = reasoner.find_patterns(pattern_type=\"dependency\")\n",
        "risk_patterns = reasoner.find_patterns(pattern_type=\"risk\")\n",
        "\n",
        "print(f\"Graph analytics: {len(centrality)} nodes analyzed for centrality\")\n",
        "print(f\"Dependency analysis: {len(dependencies)} portfolio dependencies identified\")\n",
        "print(f\"Risk modeling: {len(risk_patterns)} risk patterns detected\")\n",
        "print(\"This cookbook emphasizes graph analytics, reasoning, and risk modeling\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 7: Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"portfolio_risk_kg.html\")\n",
        "\n",
        "print(\"Risk assessment analysis complete\")\n",
        "print(\"Emphasizes: Graph analytics, portfolio risk modeling, market simulation, dependency analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
