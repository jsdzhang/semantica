{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/blockchain/02_Transaction_Network_Analysis.ipynb)\n",
        "\n",
        "# Transaction Network Analysis - Pattern Detection & Graph Analytics\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **blockchain transaction network analysis** using Semantica with focus on **pattern detection**, **network analytics**, and **real-time processing**. The pipeline analyzes blockchain transaction networks to detect patterns, identify whale movements, and analyze token flows.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Pattern Detection**: Emphasizes graph analytics for transaction pattern recognition\n",
        "- **Network Analytics**: Uses centrality measures and community detection\n",
        "- **Real-Time Processing**: Demonstrates stream processing capabilities\n",
        "- **Whale Tracking**: Identifies large transaction movements\n",
        "- **Flow Analysis**: Analyzes token flows through the network\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Blockchain Data Ingestion (Stream/File)\n",
        "3. **Phase 2**: Transaction Entity Extraction\n",
        "4. **Phase 3**: Transaction Network Graph Construction\n",
        "5. **Phase 4**: Graph Analytics (Centrality, Communities)\n",
        "6. **Phase 5**: Pattern Detection & Whale Tracking\n",
        "7. **Phase 6**: Flow Analysis\n",
        "8. **Phase 7**: Visualization & Export\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.kg import GraphAnalytics\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"Transaction_Network_Analysis\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\"}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "print(\"Configured for transaction network analysis with graph analytics focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Real Data Ingestion (Blockchain API Structure)\n",
        "\n",
        "Ingest blockchain transaction data using WebIngestor for API structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import WebIngestor, FileIngestor\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Option 1: Ingest from blockchain API (simulated structure)\n",
        "# In production, use actual blockchain.com or Etherscan API\n",
        "blockchain_api_url = \"https://api.blockchain.info/stats\"  # Example API endpoint\n",
        "\n",
        "try:\n",
        "    web_ingestor = WebIngestor()\n",
        "    # Ingest from blockchain API\n",
        "    api_documents = web_ingestor.ingest(blockchain_api_url, method=\"url\")\n",
        "    print(f\"Ingested {len(api_documents)} documents from blockchain API\")\n",
        "    documents = api_documents\n",
        "except Exception as e:\n",
        "    print(f\"API ingestion failed (using sample data): {e}\")\n",
        "    # Fallback: Sample blockchain transaction data\n",
        "    tx_data = \"\"\"\n",
        "    Transaction 0x123 transfers 1000 ETH from wallet A to wallet B.\n",
        "    Transaction 0x456 transfers 500 BTC from wallet C to wallet D.\n",
        "    Large transaction 0x789 moves 10000 ETH (whale movement) from wallet E to wallet F.\n",
        "    Transaction 0xabc transfers 200 USDT from wallet G to wallet H.\n",
        "    Transaction 0xdef transfers 5000 ETH from wallet I to wallet J.\n",
        "    \"\"\"\n",
        "    with open(\"data/transactions.txt\", \"w\") as f:\n",
        "        f.write(tx_data)\n",
        "    documents = FileIngestor().ingest(\"data/transactions.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 2: Text Normalization & Deduplication\n",
        "\n",
        "Normalize addresses and detect duplicate transactions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.deduplication import DuplicateDetector\n",
        "\n",
        "# Normalize addresses and transaction data\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Build transaction network graph\n",
        "result = core.build_knowledge_base(\n",
        "    sources=normalized_documents,\n",
        "    custom_entity_types=[\"Transaction\", \"Wallet\", \"Address\", \"Block\", \"Flow\"],\n",
        "    graph=True\n",
        ")\n",
        "\n",
        "# Detect duplicate transactions\n",
        "entities = result[\"entities\"]\n",
        "transactions = [e for e in entities if e.get(\"type\") == \"Transaction\" or \"transaction\" in e.get(\"type\", \"\").lower()]\n",
        "\n",
        "detector = DuplicateDetector()\n",
        "duplicates = detector.detect_duplicates(transactions, threshold=0.9)\n",
        "deduplicated_transactions = detector.resolve_duplicates(transactions, duplicates)\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "print(f\"Built transaction network with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"Deduplicated: {len(transactions)} -> {len(deduplicated_transactions)} unique transactions\")\n",
        "print(\"Focus: Pattern detection, network analytics, real-time processing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform network analytics\n",
        "analytics = GraphAnalytics(kg)\n",
        "centrality = analytics.calculate_centrality(method=\"degree\")\n",
        "communities = analytics.detect_communities()\n",
        "\n",
        "# Detect whale movements (large transactions)\n",
        "whale_wallets = [e for e in kg.get(\"entities\", []) \n",
        "                 if e.get(\"type\") == \"Wallet\" and \n",
        "                 any(\"large\" in str(r.get(\"predicate\", \"\")).lower() \n",
        "                     for r in kg.get(\"relationships\", []) \n",
        "                     if r.get(\"source\") == e.get(\"id\"))]\n",
        "\n",
        "print(f\"Network analytics: {len(communities)} communities, {len(centrality)} central nodes\")\n",
        "print(f\"Whale tracking: {len(whale_wallets)} large transaction wallets identified\")\n",
        "print(\"\\n=== Pipeline Summary ===\")\n",
        "print(f\"✓ Ingested {len(documents)} documents from blockchain API\")\n",
        "print(f\"✓ Normalized {len(normalized_documents)} documents\")\n",
        "print(f\"✓ Deduplicated {len(transactions)} transactions to {len(deduplicated_transactions)} unique\")\n",
        "print(f\"✓ Detected {len(communities)} communities and {len(whale_wallets)} whale wallets\")\n",
        "print(f\"✓ This cookbook emphasizes graph analytics and pattern detection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 6-7: Visualization & Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"transaction_network.html\", layout=\"force-directed\")\n",
        "\n",
        "print(\"Transaction network analysis complete\")\n",
        "print(\"Emphasizes: Graph analytics, pattern detection, network analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
