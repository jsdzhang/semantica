{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/blockchain/01_DeFi_Protocol_Intelligence.ipynb)\n",
        "\n",
        "# DeFi Protocol Intelligence - Risk Assessment & Ontology Reasoning\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **DeFi protocol intelligence** using Semantica with focus on **risk assessment**, **ontology-based reasoning**, and **relationship analysis**. The pipeline ingests DeFi data, extracts protocol entities, builds DeFi knowledge graphs, and assesses risks using graph reasoning.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Risk Assessment Focus**: Emphasizes KG construction and reasoning for risk evaluation\n",
        "- **Ontology-Based Reasoning**: Uses domain ontologies for DeFi protocol analysis\n",
        "- **Relationship Analysis**: Analyzes protocol relationships and dependencies\n",
        "- **Yield Optimization**: Identifies optimization opportunities through graph analysis\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: DeFi Data Ingestion\n",
        "3. **Phase 2**: Protocol Entity Extraction\n",
        "4. **Phase 3**: DeFi Knowledge Graph Construction\n",
        "5. **Phase 4**: Ontology Generation & Reasoning\n",
        "6. **Phase 5**: Risk Assessment Analysis\n",
        "7. **Phase 6**: Relationship Analysis\n",
        "8. **Phase 7**: Visualization & Export\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.ontology import OntologyGenerator\n",
        "from semantica.reasoning import GraphReasoner\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"DeFi_Protocol_Intelligence\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\"},\n",
        "    \"ontology\": {\"base_uri\": \"https://defi.example.org/ontology/\"}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "print(\"Configured for DeFi protocol intelligence with ontology reasoning focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Real Data Ingestion (CoinDesk RSS Feed)\n",
        "\n",
        "Ingest DeFi protocol data from CoinDesk RSS feeds using FeedIngestor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FeedIngestor, FileIngestor\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Option 1: Ingest from CoinDesk RSS feed (real data source)\n",
        "coindesk_rss_url = \"https://www.coindesk.com/arc/outboundfeeds/rss/\"\n",
        "\n",
        "try:\n",
        "    feed_ingestor = FeedIngestor()\n",
        "    feed_documents = feed_ingestor.ingest(coindesk_rss_url, method=\"rss\")\n",
        "    print(f\"Ingested {len(feed_documents)} documents from CoinDesk RSS feed\")\n",
        "    documents = feed_documents\n",
        "except Exception as e:\n",
        "    print(f\"RSS feed ingestion failed (using sample data): {e}\")\n",
        "    defi_data = \"\"\"\n",
        "    Uniswap is a decentralized exchange protocol with high liquidity pools.\n",
        "    Aave is a lending protocol that offers variable interest rates.\n",
        "    Compound is a money market protocol for lending and borrowing.\n",
        "    MakerDAO uses collateralized debt positions (CDPs) for stablecoin generation.\n",
        "    \"\"\"\n",
        "    with open(\"data/defi_protocols.txt\", \"w\") as f:\n",
        "        f.write(defi_data)\n",
        "    documents = FileIngestor().ingest(\"data/defi_protocols.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 2: Text Normalization & Advanced Chunking\n",
        "\n",
        "Normalize protocol names and use ontology-aware chunking to preserve protocol concepts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter\n",
        "\n",
        "# Normalize protocol names and text\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Use ontology-aware chunking to preserve protocol ontology concepts\n",
        "# This will be applied after ontology generation\n",
        "splitter = TextSplitter(method=\"recursive\", chunk_size=1000, chunk_overlap=200)\n",
        "chunked_docs = []\n",
        "for doc_text in normalized_documents:\n",
        "    chunks = splitter.split(doc_text)\n",
        "    chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
        "\n",
        "print(f\"Created {len(chunked_docs)} chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 3: Knowledge Graph Construction & Conflict Detection\n",
        "\n",
        "Build DeFi knowledge graph and detect conflicts from multiple sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.conflicts import ConflictDetector\n",
        "\n",
        "# Build DeFi knowledge graph\n",
        "result = core.build_knowledge_base(\n",
        "    sources=chunked_docs,\n",
        "    custom_entity_types=[\"Protocol\", \"Token\", \"Pool\", \"Transaction\", \"Risk\"],\n",
        "    graph=True\n",
        ")\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "entities = result[\"entities\"]\n",
        "\n",
        "# Detect conflicts from multiple sources\n",
        "detector = ConflictDetector()\n",
        "conflicts = detector.detect_conflicts(entities, kg.get(\"relationships\", []))\n",
        "\n",
        "print(f\"Built DeFi KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"Detected {len(conflicts)} conflicts from multiple sources\")\n",
        "\n",
        "# Resolve conflicts using highest confidence strategy\n",
        "if conflicts:\n",
        "    resolved = detector.resolve_conflicts(conflicts, strategy=\"highest_confidence\")\n",
        "    print(f\"Resolved {len(resolved)} conflicts\")\n",
        "print(\"Focus: Risk assessment, ontology reasoning, relationship analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate DeFi ontology\n",
        "ontology_gen = OntologyGenerator(base_uri=\"https://defi.example.org/ontology/\")\n",
        "ontology = ontology_gen.generate_from_graph(kg)\n",
        "\n",
        "# Apply ontology-aware chunking using the generated ontology\n",
        "ontology_splitter = TextSplitter(\n",
        "    method=\"ontology_aware\",\n",
        "    ontology=ontology,\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Re-chunk with ontology awareness\n",
        "ontology_chunks = []\n",
        "for doc_text in normalized_documents[:3]:  # Apply to subset for demonstration\n",
        "    chunks = ontology_splitter.split(doc_text)\n",
        "    ontology_chunks.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
        "\n",
        "print(f\"Generated DeFi ontology with {len(ontology.get('classes', []))} classes\")\n",
        "print(f\"Created {len(ontology_chunks)} ontology-aware chunks\")\n",
        "\n",
        "# Use reasoning for risk assessment\n",
        "reasoner = GraphReasoner(kg)\n",
        "risk_patterns = reasoner.find_patterns(pattern_type=\"risk\")\n",
        "\n",
        "print(f\"Identified {len(risk_patterns)} risk patterns\")\n",
        "print(\"This cookbook emphasizes ontology reasoning, ontology-aware chunking, and risk assessment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 5-6: Visualization & Export\n",
        "\n",
        "Visualize DeFi protocol knowledge graph and export ontology.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "from semantica.export import GraphExporter\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"defi_protocol_kg.html\")\n",
        "\n",
        "exporter = GraphExporter()\n",
        "exporter.export(kg, format=\"rdf\", output_path=\"defi_ontology.ttl\")\n",
        "\n",
        "print(\"DeFi protocol intelligence analysis complete\")\n",
        "print(\"\\n=== Pipeline Summary ===\")\n",
        "print(f\"✓ Ingested {len(documents)} documents from CoinDesk RSS feed\")\n",
        "print(f\"✓ Normalized {len(normalized_documents)} documents\")\n",
        "print(f\"✓ Detected and resolved {len(conflicts)} conflicts\")\n",
        "print(f\"✓ Generated ontology with {len(ontology.get('classes', []))} classes\")\n",
        "print(f\"✓ Created {len(ontology_chunks)} ontology-aware chunks\")\n",
        "print(f\"✓ Identified {len(risk_patterns)} risk patterns\")\n",
        "print(f\"✓ Emphasizes: KG construction, ontology reasoning, ontology-aware chunking, risk assessment\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
