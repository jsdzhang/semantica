{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/cybersecurity/02_Threat_Intelligence_Hybrid_RAG.ipynb)\n",
        "\n",
        "# Threat Intelligence Hybrid RAG - Vector + Graph Retrieval\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **threat intelligence hybrid RAG** using Semantica with focus on **hybrid search**, **vector + graph retrieval**, and **context-aware queries**. The pipeline combines vector search with temporal knowledge graphs for advanced threat intelligence querying.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Hybrid RAG**: Combines vector similarity search with knowledge graph traversal\n",
        "- **Vector + Graph Retrieval**: Uses both vector embeddings and graph relationships\n",
        "- **Context-Aware Queries**: Provides context-aware retrieval for threat intelligence\n",
        "- **Temporal Knowledge Graphs**: Builds temporal KGs for threat timeline analysis\n",
        "- **Multi-hop Reasoning**: Follows relationships across the graph for deeper context\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Threat Intelligence Data Ingestion\n",
        "3. **Phase 2**: Entity Extraction (IOC, Campaign, Threat, Actor, TTP)\n",
        "4. **Phase 3**: Temporal Knowledge Graph Construction\n",
        "5. **Phase 4**: Embedding Generation & Vector Store\n",
        "6. **Phase 5**: Hybrid Search Setup (Vector + Graph)\n",
        "7. **Phase 6**: Context-Aware Query System\n",
        "8. **Phase 7**: Visualization & Export\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu groq sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.vector_store import VectorStore\n",
        "from semantica.context import AgentContext\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"Threat_Intelligence_Hybrid_RAG\",\n",
        "    \"embedding\": {\"provider\": \"sentence_transformers\", \"model\": \"all-MiniLM-L6-v2\"},\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"vector_store\": {\"provider\": \"faiss\", \"dimension\": 384},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\", \"temporal\": True}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=384)\n",
        "print(\"Configured for threat intelligence hybrid RAG\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Real Data Ingestion (Security RSS Feeds)\n",
        "\n",
        "Ingest threat intelligence data from multiple security RSS feeds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FeedIngestor, FileIngestor\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Ingest from multiple security RSS feeds (real data sources)\n",
        "security_feeds = [\n",
        "    \"https://www.us-cert.gov/ncas/alerts.xml\",  # US-CERT alerts\n",
        "    # Add more security feeds as needed\n",
        "]\n",
        "\n",
        "documents = []\n",
        "for feed_url in security_feeds:\n",
        "    try:\n",
        "        feed_ingestor = FeedIngestor()\n",
        "        feed_documents = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
        "        print(f\"Ingested {len(feed_documents)} documents from {feed_url}\")\n",
        "        documents.extend(feed_documents)\n",
        "    except Exception as e:\n",
        "        print(f\"Feed ingestion failed for {feed_url}: {e}\")\n",
        "\n",
        "# Fallback: Sample threat intelligence data\n",
        "if not documents:\n",
        "    threat_data = \"\"\"\n",
        "    IOC: IP address 192.168.1.50 associated with APT28 campaign.\n",
        "    Threat actor APT28 uses TTP: Spear phishing and credential harvesting.\n",
        "    Campaign Operation GhostShell targets financial institutions.\n",
        "    Malware sample hash: abc123def456 linked to APT28 infrastructure.\n",
        "    IOC: Domain example-malicious.com linked to APT29 operations.\n",
        "    \"\"\"\n",
        "    with open(\"data/threat_intel.txt\", \"w\") as f:\n",
        "        f.write(threat_data)\n",
        "    documents = FileIngestor().ingest(\"data/threat_intel.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 2: Text Normalization & Advanced Chunking\n",
        "\n",
        "Normalize IOC data and use entity-aware/relation-aware chunking for threat relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter, EntityAwareChunker, RelationAwareChunker\n",
        "from semantica.deduplication import DuplicateDetector\n",
        "\n",
        "# Normalize IOC data and threat intelligence\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Use entity-aware chunking to preserve threat entity boundaries for GraphRAG\n",
        "entity_splitter = TextSplitter(\n",
        "    method=\"entity_aware\",\n",
        "    ner_method=\"llm\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Alternative: relation-aware chunking to preserve threat relationship triplets\n",
        "relation_splitter = TextSplitter(\n",
        "    method=\"relation_aware\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Chunk with entity-aware (preserves IOC, Actor, Campaign boundaries)\n",
        "chunked_docs = []\n",
        "for doc_text in normalized_documents:\n",
        "    chunks = entity_splitter.split(doc_text)\n",
        "    chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
        "\n",
        "print(f\"Created {len(chunked_docs)} entity-aware chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 3: Knowledge Graph Construction & Deduplication\n",
        "\n",
        "Build temporal knowledge graph and deduplicate threat entities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build knowledge base with both vectors and graph\n",
        "result = core.build_knowledge_base(\n",
        "    sources=chunked_docs,\n",
        "    custom_entity_types=[\"IOC\", \"Campaign\", \"Threat\", \"Actor\", \"TTP\", \"Malware\"],\n",
        "    embeddings=True,\n",
        "    graph=True,\n",
        "    temporal=True\n",
        ")\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "entities = result[\"entities\"]\n",
        "embeddings_data = result[\"embeddings\"]\n",
        "\n",
        "# Deduplicate threat entities (IOCs, actors, campaigns)\n",
        "iocs = [e for e in entities if e.get(\"type\") == \"IOC\" or \"ioc\" in e.get(\"type\", \"\").lower()]\n",
        "actors = [e for e in entities if e.get(\"type\") == \"Actor\" or \"actor\" in e.get(\"type\", \"\").lower()]\n",
        "\n",
        "detector = DuplicateDetector()\n",
        "ioc_duplicates = detector.detect_duplicates(iocs, threshold=0.9)\n",
        "actor_duplicates = detector.detect_duplicates(actors, threshold=0.9)\n",
        "\n",
        "deduplicated_iocs = detector.resolve_duplicates(iocs, ioc_duplicates)\n",
        "deduplicated_actors = detector.resolve_duplicates(actors, actor_duplicates)\n",
        "\n",
        "print(f\"Built hybrid system: {len(kg.get('entities', []))} entities, {len(embeddings_data['vectors'])} vectors\")\n",
        "print(f\"Deduplicated: {len(iocs)} -> {len(deduplicated_iocs)} unique IOCs\")\n",
        "print(f\"Deduplicated: {len(actors)} -> {len(deduplicated_actors)} unique actors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 4: Vector Store Population\n",
        "\n",
        "Store embeddings in vector store for hybrid retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store embeddings in vector store\n",
        "vector_store.store_vectors(\n",
        "    vectors=embeddings_data[\"vectors\"],\n",
        "    metadata=embeddings_data[\"metadata\"]\n",
        ")\n",
        "\n",
        "print(f\"Stored {len(embeddings_data['vectors'])} embeddings in vector store\")\n",
        "print(\"Focus: Hybrid RAG, vector + graph retrieval, context-aware queries\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.context import AgentContext\n",
        "from semantica.embeddings import EmbeddingGenerator\n",
        "\n",
        "# Initialize enhanced GraphRAG context\n",
        "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
        "\n",
        "# Example hybrid query\n",
        "query = \"What threats are associated with APT28?\"\n",
        "print(f\"Query: {query}\\n\")\n",
        "\n",
        "# Generate query embedding\n",
        "embedding_gen = EmbeddingGenerator(provider=\"sentence_transformers\", model=\"all-MiniLM-L6-v2\")\n",
        "query_embedding = embedding_gen.generate_embeddings([query])[0]\n",
        "\n",
        "# Vector search\n",
        "vector_results = vector_store.search_vectors(query_embedding, k=5)\n",
        "\n",
        "# Enhanced GraphRAG retrieval with graph expansion\n",
        "graph_context = context.retrieve(\n",
        "    query,\n",
        "    max_results=10,\n",
        "    use_graph=True,  # Enable graph traversal\n",
        "    expand_graph=True,  # Expand graph relationships\n",
        "    include_entities=True,  # Include related entities\n",
        "    include_relationships=True  # Include relationships\n",
        ")\n",
        "\n",
        "print(f\"Hybrid RAG results:\")\n",
        "print(f\"  - Vector search: {len(vector_results)} matches\")\n",
        "print(f\"  - GraphRAG retrieval: {len(graph_context)} context items with graph expansion\")\n",
        "print(f\"\\nTop GraphRAG results:\")\n",
        "for i, result in enumerate(graph_context[:3], 1):\n",
        "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
        "    print(f\"   Content: {result.get('content', '')[:150]}...\")\n",
        "    if result.get('related_entities'):\n",
        "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
        "print(\"\\nThis cookbook emphasizes enhanced GraphRAG with entity-aware chunking and deduplication\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 7: Visualization & Summary\n",
        "\n",
        "Visualize threat intelligence knowledge graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"threat_intelligence_kg.html\")\n",
        "\n",
        "print(\"Threat intelligence hybrid RAG analysis complete\")\n",
        "print(\"\\n=== Pipeline Summary ===\")\n",
        "print(f\"✓ Ingested {len(documents)} documents from security RSS feeds\")\n",
        "print(f\"✓ Normalized {len(normalized_documents)} documents\")\n",
        "print(f\"✓ Created {len(chunked_docs)} entity-aware chunks\")\n",
        "print(f\"✓ Deduplicated IOCs and actors\")\n",
        "print(f\"✓ Built temporal KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"✓ Stored {len(embeddings_data['vectors'])} embeddings\")\n",
        "print(f\"✓ Enhanced GraphRAG with graph expansion enabled\")\n",
        "print(f\"✓ Emphasizes: Hybrid RAG, entity-aware chunking, vector + graph retrieval, context-aware queries\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
