{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/cybersecurity/01_Real_Time_Anomaly_Detection.ipynb)\n",
        "\n",
        "# Real-Time Anomaly Detection - Stream Processing & Temporal KGs\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **real-time anomaly detection** using Semantica with focus on **stream ingestion**, **temporal knowledge graphs**, and **pattern detection**. The pipeline streams security logs in real-time, builds temporal knowledge graphs, and detects anomalies using pattern detection.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Stream Processing**: Emphasizes real-time log streaming and processing\n",
        "- **Temporal Knowledge Graphs**: Builds temporal KGs to track events over time\n",
        "- **Pattern Detection**: Uses graph patterns to identify anomalies\n",
        "- **Automated Alerting**: Generates alerts for detected anomalies\n",
        "- **Real-Time Processing**: Demonstrates stream ingestion capabilities\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Stream Security Log Ingestion\n",
        "3. **Phase 2**: Real-Time Log Parsing\n",
        "4. **Phase 3**: Entity Extraction (Log, Event, IP, User, Alert)\n",
        "5. **Phase 4**: Temporal Knowledge Graph Construction\n",
        "6. **Phase 5**: Pattern Detection\n",
        "7. **Phase 6**: Anomaly Detection\n",
        "8. **Phase 7**: Alert Generation & Visualization\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.ingest import StreamIngestor\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"Real_Time_Anomaly_Detection\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\", \"temporal\": True}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "print(\"Configured for real-time anomaly detection with stream processing focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Real Data Ingestion (CVE RSS Feed & Kafka Stream)\n",
        "\n",
        "Ingest security data from CVE RSS feeds and simulated Kafka streams.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FeedIngestor, StreamIngestor, FileIngestor\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Option 1: Ingest from CVE RSS feed (real data source)\n",
        "cve_rss_url = \"https://cve.mitre.org/data/downloads/allitems.xml\"  # CVE feed\n",
        "\n",
        "documents = []\n",
        "try:\n",
        "    feed_ingestor = FeedIngestor()\n",
        "    feed_documents = feed_ingestor.ingest(cve_rss_url, method=\"rss\")\n",
        "    print(f\"Ingested {len(feed_documents)} documents from CVE RSS feed\")\n",
        "    documents.extend(feed_documents)\n",
        "except Exception as e:\n",
        "    print(f\"CVE RSS feed ingestion failed: {e}\")\n",
        "\n",
        "# Option 2: Simulate Kafka stream (in production, use actual Kafka)\n",
        "# stream_ingestor = StreamIngestor()\n",
        "# stream_documents = stream_ingestor.ingest(\"kafka://localhost:9092/security-logs\", method=\"kafka\")\n",
        "\n",
        "# Fallback: Sample security log stream data\n",
        "security_logs = \"\"\"\n",
        "2024-01-01 10:00:00 - Login attempt from IP 192.168.1.100 user admin\n",
        "2024-01-01 10:01:00 - Failed login from IP 192.168.1.100 user admin\n",
        "2024-01-01 10:02:00 - Multiple failed logins from IP 192.168.1.100 user admin\n",
        "2024-01-01 10:03:00 - Unusual activity detected from IP 192.168.1.100\n",
        "2024-01-01 10:04:00 - Alert: Potential brute force attack from IP 192.168.1.100\n",
        "2024-01-01 10:05:00 - Login attempt from IP 192.168.1.101 user test\n",
        "\"\"\"\n",
        "\n",
        "with open(\"data/security_logs.txt\", \"w\") as f:\n",
        "    f.write(security_logs)\n",
        "\n",
        "stream_documents = FileIngestor().ingest(\"data/security_logs.txt\")\n",
        "documents.extend(stream_documents)\n",
        "print(f\"Added {len(stream_documents)} documents from simulated stream\")\n",
        "print(f\"Total documents: {len(documents)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 2: Text Normalization & Advanced Chunking\n",
        "\n",
        "Normalize log data and use sentence/recursive chunking for structured logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter\n",
        "\n",
        "# Normalize log data\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Use sentence chunking for log line boundaries (structured logs)\n",
        "splitter = TextSplitter(method=\"sentence\", chunk_size=500, chunk_overlap=50)\n",
        "# Alternative: recursive for hierarchical log structures\n",
        "# splitter = TextSplitter(method=\"recursive\", chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "chunked_docs = []\n",
        "for doc_text in normalized_documents:\n",
        "    chunks = splitter.split(doc_text)\n",
        "    chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
        "\n",
        "print(f\"Created {len(chunked_docs)} chunks using sentence chunking\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 3-4: Temporal Knowledge Graph Construction\n",
        "\n",
        "Build full temporal KG with TemporalGraphQuery capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphBuilder, TemporalGraphQuery\n",
        "\n",
        "# Build temporal knowledge graph\n",
        "result = core.build_knowledge_base(\n",
        "    sources=chunked_docs,\n",
        "    custom_entity_types=[\"Log\", \"Event\", \"IP\", \"User\", \"Alert\", \"Attack\"],\n",
        "    graph=True,\n",
        "    temporal=True\n",
        ")\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "\n",
        "# Initialize temporal graph query engine\n",
        "temporal_query = TemporalGraphQuery(\n",
        "    enable_temporal_reasoning=True,\n",
        "    temporal_granularity=\"minute\"  # Fine-grained for real-time logs\n",
        ")\n",
        "\n",
        "# Query graph at specific time point\n",
        "query_results = temporal_query.query_at_time(\n",
        "    kg,\n",
        "    query={\"type\": \"Alert\"},\n",
        "    at_time=\"2024-01-01 10:04:00\"\n",
        ")\n",
        "\n",
        "# Analyze temporal evolution\n",
        "evolution = temporal_query.analyze_evolution(kg)\n",
        "\n",
        "print(f\"Built temporal KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"Temporal queries: {len(query_results)} alerts at query time\")\n",
        "print(\"Focus: Stream processing, temporal KGs, pattern detection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.reasoning import GraphReasoner\n",
        "\n",
        "# Detect anomaly patterns (e.g., multiple failed logins)\n",
        "reasoner = GraphReasoner(kg)\n",
        "anomaly_patterns = reasoner.find_patterns(pattern_type=\"anomaly\")\n",
        "\n",
        "# Temporal pattern detection\n",
        "temporal_patterns = temporal_query.detect_temporal_patterns(kg, pattern_type=\"sequence\")\n",
        "\n",
        "# Identify suspicious IPs\n",
        "suspicious_ips = [e for e in kg.get(\"entities\", []) \n",
        "                  if e.get(\"type\") == \"IP\" and \n",
        "                  any(\"alert\" in str(r.get(\"predicate\", \"\")).lower() \n",
        "                      for r in kg.get(\"relationships\", []) \n",
        "                      if r.get(\"target\") == e.get(\"id\"))]\n",
        "\n",
        "print(f\"Pattern detection: {len(anomaly_patterns)} anomaly patterns found\")\n",
        "print(f\"Temporal patterns: {len(temporal_patterns)} temporal patterns detected\")\n",
        "print(f\"Anomaly detection: {len(suspicious_ips)} suspicious IPs identified\")\n",
        "print(\"This cookbook emphasizes stream processing, temporal KGs, and pattern detection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 7: Visualization & Alert Generation\n",
        "\n",
        "Visualize temporal knowledge graph with anomaly alerts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"anomaly_detection_kg.html\", layout=\"temporal\")\n",
        "\n",
        "print(\"Real-time anomaly detection analysis complete\")\n",
        "print(\"\\n=== Pipeline Summary ===\")\n",
        "print(f\"✓ Ingested {len(documents)} documents from CVE RSS feed and stream\")\n",
        "print(f\"✓ Normalized {len(normalized_documents)} documents\")\n",
        "print(f\"✓ Created {len(chunked_docs)} chunks using sentence chunking\")\n",
        "print(f\"✓ Built temporal KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"✓ Detected {len(anomaly_patterns)} anomaly patterns and {len(suspicious_ips)} suspicious IPs\")\n",
        "print(f\"✓ Emphasizes: Stream processing, temporal KGs, pattern detection\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
