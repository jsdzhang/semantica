{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/intelligence/02_Intelligence_Analysis_Orchestrator_Worker.ipynb)\n",
        "\n",
        "# Intelligence Analysis Orchestrator-Worker - Parallel Processing\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **intelligence analysis using orchestrator-worker pattern** with focus on **parallel processing**, **pipeline orchestration**, and **multi-source integration**. The pipeline uses the Orchestrator-Worker pattern to coordinate parallel processing of OSINT feeds, threat intelligence, and geospatial data.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Parallel Processing**: Coordinates parallel processing of multiple intelligence sources\n",
        "- **Pipeline Orchestration**: Uses pipeline module for workflow management\n",
        "- **Multi-Source Integration**: Integrates OSINT feeds, threat intelligence, and geospatial data\n",
        "- **Orchestrator-Worker Pattern**: Demonstrates distributed processing architecture\n",
        "- **Hybrid RAG**: Combines multiple intelligence sources for comprehensive analysis\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Multi-Source Intelligence Ingestion (OSINT, Threat Intel, Geospatial)\n",
        "3. **Phase 2**: Parallel Processing Setup (Orchestrator-Worker)\n",
        "4. **Phase 3**: Entity Extraction (Source, Entity, Event, Location, Timeframe)\n",
        "5. **Phase 4**: Pipeline Orchestration\n",
        "6. **Phase 5**: Multi-Source Correlation\n",
        "7. **Phase 6**: Hybrid RAG Query System\n",
        "8. **Phase 7**: Visualization & Export\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.pipeline import PipelineOrchestrator\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"Intelligence_Analysis_Orchestrator_Worker\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\"}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "orchestrator = PipelineOrchestrator()\n",
        "print(\"Configured for intelligence analysis with orchestrator-worker pattern focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Real Data Ingestion (Multiple RSS Feeds with Pipeline Orchestrator)\n",
        "\n",
        "Ingest intelligence data from multiple RSS feeds using PipelineOrchestrator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FeedIngestor, FileIngestor\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Define multiple RSS feeds for parallel ingestion\n",
        "intelligence_feeds = {\n",
        "    \"osint\": \"https://www.us-cert.gov/ncas/alerts.xml\",  # OSINT feed\n",
        "    \"threat\": \"https://www.us-cert.gov/ncas/alerts.xml\",  # Threat intel feed (example)\n",
        "    # Add more feeds as needed\n",
        "}\n",
        "\n",
        "# Use PipelineOrchestrator for parallel ingestion\n",
        "documents = []\n",
        "for feed_name, feed_url in intelligence_feeds.items():\n",
        "    try:\n",
        "        feed_ingestor = FeedIngestor()\n",
        "        # Add worker to orchestrator\n",
        "        orchestrator.add_worker(f\"{feed_name}_worker\", feed_ingestor, source=feed_url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to add worker for {feed_name}: {e}\")\n",
        "\n",
        "# Execute parallel ingestion\n",
        "try:\n",
        "    results = orchestrator.execute_parallel()\n",
        "    for result in results:\n",
        "        documents.extend(result.get(\"documents\", []))\n",
        "    print(f\"Ingested {len(documents)} documents from parallel feeds\")\n",
        "except Exception as e:\n",
        "    print(f\"Parallel ingestion failed: {e}\")\n",
        "    # Fallback: Sequential ingestion\n",
        "    for feed_name, feed_url in intelligence_feeds.items():\n",
        "        try:\n",
        "            feed_ingestor = FeedIngestor()\n",
        "            feed_documents = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
        "            documents.extend(feed_documents)\n",
        "            print(f\"Ingested {len(feed_documents)} documents from {feed_name}\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Feed ingestion failed for {feed_name}: {e2}\")\n",
        "\n",
        "# Fallback: Sample data\n",
        "if not documents:\n",
        "    osint_data = \"OSINT: Public records show connection between Entity A and Location X.\"\n",
        "    threat_data = \"Threat Intel: Threat actor group Y operates in Region Z.\"\n",
        "    geo_data = \"Geospatial: Activity detected at coordinates 40.7128, -74.0060.\"\n",
        "    with open(\"data/intelligence.txt\", \"w\") as f:\n",
        "        f.write(f\"{osint_data}\\n{threat_data}\\n{geo_data}\")\n",
        "    documents = FileIngestor().ingest(\"data/intelligence.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 2: Text Normalization & Conflict Detection\n",
        "\n",
        "Normalize multi-source intelligence data and detect conflicts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.conflicts import ConflictDetector\n",
        "\n",
        "# Normalize multi-source intelligence data\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Build intelligence knowledge graph\n",
        "result = core.build_knowledge_base(\n",
        "    sources=normalized_documents,\n",
        "    custom_entity_types=[\"Source\", \"Entity\", \"Event\", \"Location\", \"Timeframe\"],\n",
        "    graph=True\n",
        ")\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "entities = result[\"entities\"]\n",
        "\n",
        "# Detect conflicts from multiple intelligence sources\n",
        "detector = ConflictDetector()\n",
        "conflicts = detector.detect_conflicts(entities, kg.get(\"relationships\", []))\n",
        "\n",
        "print(f\"Built intelligence KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"Detected {len(conflicts)} conflicts from multiple sources\")\n",
        "if conflicts:\n",
        "    resolved = detector.resolve_conflicts(conflicts, strategy=\"highest_confidence\")\n",
        "    print(f\"Resolved {len(resolved)} conflicts\")\n",
        "print(\"Focus: Parallel processing, pipeline orchestration, multi-source integration\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlate multi-source intelligence\n",
        "from semantica.reasoning import GraphReasoner\n",
        "\n",
        "reasoner = GraphReasoner(kg)\n",
        "correlations = reasoner.find_correlations(\n",
        "    source_types=[\"Source\"],\n",
        "    target_types=[\"Entity\", \"Location\"]\n",
        ")\n",
        "\n",
        "print(f\"Multi-source correlation: {len(correlations)} correlations found\")\n",
        "print(\"\\n=== Pipeline Summary ===\")\n",
        "print(f\"✓ Ingested {len(documents)} documents from multiple RSS feeds using PipelineOrchestrator\")\n",
        "print(f\"✓ Normalized {len(normalized_documents)} documents\")\n",
        "print(f\"✓ Detected and resolved {len(conflicts)} conflicts\")\n",
        "print(f\"✓ This cookbook emphasizes parallel processing, PipelineOrchestrator, and multi-source integration\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 7: Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"intelligence_analysis.html\")\n",
        "\n",
        "print(\"Intelligence analysis (orchestrator-worker) complete\")\n",
        "print(\"Emphasizes: Parallel processing, pipeline orchestration, multi-source integration\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
