{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/biomedical/02_Genomic_Variant_Analysis.ipynb)\n",
        "\n",
        "# Genomic Variant Analysis - Graph Analytics & Pathway Analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **genomic variant analysis** using Semantica with focus on **graph analytics**, **pathway analysis**, and **temporal knowledge graphs**. The pipeline analyzes genomic data to extract variant entities, build temporal genomic knowledge graphs, and analyze disease associations through reasoning.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Graph Analytics Focus**: Emphasizes graph reasoning, centrality measures, and pathway analysis\n",
        "- **Temporal Analysis**: Builds temporal genomic knowledge graphs to track variant evolution\n",
        "- **Disease Association**: Analyzes relationships between variants, genes, and diseases\n",
        "- **Pathway Analysis**: Uses graph traversal to identify biological pathways\n",
        "- **Impact Prediction**: Predicts variant impact using graph-based reasoning\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Genomic Data Ingestion\n",
        "3. **Phase 2**: Variant Entity Extraction\n",
        "4. **Phase 3**: Temporal Knowledge Graph Construction\n",
        "5. **Phase 4**: Graph Analytics (Centrality, Communities)\n",
        "6. **Phase 5**: Pathway Analysis & Reasoning\n",
        "7. **Phase 6**: Disease Association Analysis\n",
        "8. **Phase 7**: Visualization & Export\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.kg import KnowledgeGraphBuilder\n",
        "from semantica.reasoning import GraphReasoner\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"Genomic_Variant_Analysis\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\", \"temporal\": True}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "print(\"Configured for genomic variant analysis with graph analytics focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Real Data Ingestion (bioRxiv RSS Feed)\n",
        "\n",
        "Ingest genomic variant data from bioRxiv RSS feeds using FeedIngestor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FeedIngestor, FileIngestor\n",
        "import os\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Option 1: Ingest from bioRxiv RSS feed (real data source)\n",
        "# bioRxiv RSS feed for genomic variant research\n",
        "biorxiv_rss_url = \"https://connect.biorxiv.org/biorxiv_xml.php?subject=genetics\"\n",
        "\n",
        "try:\n",
        "    feed_ingestor = FeedIngestor()\n",
        "    # Ingest from bioRxiv RSS feed\n",
        "    feed_documents = feed_ingestor.ingest(biorxiv_rss_url, method=\"rss\")\n",
        "    print(f\"Ingested {len(feed_documents)} documents from bioRxiv RSS feed\")\n",
        "    documents = feed_documents\n",
        "except Exception as e:\n",
        "    print(f\"RSS feed ingestion failed (using sample data): {e}\")\n",
        "    # Fallback: Sample genomic variant data\n",
        "    variant_data = \"\"\"\n",
        "    Variant rs699 is located in the AGT gene and associated with hypertension.\n",
        "    Variant rs7412 in APOE gene is linked to Alzheimer's disease risk.\n",
        "    BRCA1 variant c.5266dupC increases breast cancer susceptibility.\n",
        "    CFTR variant F508del causes cystic fibrosis.\n",
        "    Variant rs1800566 in NAT2 gene affects drug metabolism.\n",
        "    \"\"\"\n",
        "    \n",
        "    with open(\"data/variants.txt\", \"w\") as f:\n",
        "        f.write(variant_data)\n",
        "    \n",
        "    documents = FileIngestor().ingest(\"data/variants.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 2: Text Normalization & Deduplication\n",
        "\n",
        "Normalize genomic data and detect duplicate variants.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.deduplication import DuplicateDetector\n",
        "\n",
        "# Normalize genomic data\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Build knowledge base first to get entities for deduplication\n",
        "result = core.build_knowledge_base(\n",
        "    sources=normalized_documents,\n",
        "    custom_entity_types=[\"Variant\", \"Gene\", \"Disease\", \"Pathway\"],\n",
        "    graph=True,\n",
        "    temporal=True\n",
        ")\n",
        "\n",
        "# Detect duplicate variants\n",
        "entities = result[\"entities\"]\n",
        "variants = [e for e in entities if e.get(\"type\") == \"Variant\" or \"variant\" in e.get(\"type\", \"\").lower()]\n",
        "\n",
        "detector = DuplicateDetector()\n",
        "duplicates = detector.detect_duplicates(variants, threshold=0.9)\n",
        "deduplicated_variants = detector.resolve_duplicates(variants, duplicates)\n",
        "\n",
        "print(f\"Detected {len(duplicates)} duplicate variant groups\")\n",
        "print(f\"Deduplicated: {len(variants)} -> {len(deduplicated_variants)} unique variants\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 3: Temporal Knowledge Graph Construction\n",
        "\n",
        "Build temporal knowledge graph with temporal query capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphBuilder, TemporalGraphQuery\n",
        "\n",
        "# Get knowledge graph from result\n",
        "kg = result[\"knowledge_graph\"]\n",
        "\n",
        "# Initialize temporal graph query engine\n",
        "temporal_query = TemporalGraphQuery(\n",
        "    enable_temporal_reasoning=True,\n",
        "    temporal_granularity=\"day\"\n",
        ")\n",
        "\n",
        "# Query graph at specific time point\n",
        "# Example: Query variants active on a specific date\n",
        "query_results = temporal_query.query_at_time(\n",
        "    kg,\n",
        "    query={\"type\": \"Variant\"},\n",
        "    at_time=\"2024-01-01\"\n",
        ")\n",
        "\n",
        "# Analyze temporal evolution\n",
        "evolution = temporal_query.analyze_evolution(kg)\n",
        "\n",
        "print(f\"Built temporal KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"Temporal queries: {len(query_results)} variants at query time\")\n",
        "print(f\"Graph emphasizes: graph analytics, pathway analysis, temporal reasoning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphAnalytics\n",
        "\n",
        "# Perform graph analytics (centrality, communities)\n",
        "analytics = GraphAnalytics(kg)\n",
        "centrality = analytics.calculate_centrality(method=\"betweenness\")\n",
        "communities = analytics.detect_communities()\n",
        "\n",
        "# Use reasoning for pathway analysis\n",
        "reasoner = GraphReasoner(kg)\n",
        "pathways = reasoner.find_paths(\n",
        "    source_type=\"Variant\",\n",
        "    target_type=\"Disease\",\n",
        "    max_hops=3\n",
        ")\n",
        "\n",
        "# Temporal pattern detection\n",
        "temporal_patterns = temporal_query.detect_temporal_patterns(kg, pattern_type=\"sequence\")\n",
        "\n",
        "print(f\"Graph analytics: {len(communities)} communities detected\")\n",
        "print(f\"Pathway analysis: {len(pathways)} variant-disease pathways found\")\n",
        "print(f\"Temporal patterns: {len(temporal_patterns)} temporal patterns detected\")\n",
        "print(\"This cookbook emphasizes graph analytics, temporal reasoning, and pathway analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 6-7: Visualization & Export\n",
        "\n",
        "Visualize temporal knowledge graph and export results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "from semantica.export import GraphExporter\n",
        "\n",
        "# Visualize temporal knowledge graph\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"genomic_variant_kg.html\", layout=\"hierarchical\")\n",
        "\n",
        "# Export graph\n",
        "exporter = GraphExporter()\n",
        "exporter.export(kg, format=\"graphml\", output_path=\"genomic_variant_kg.graphml\")\n",
        "\n",
        "print(\"Visualization and export complete\")\n",
        "print(\"\\n=== Pipeline Summary ===\")\n",
        "print(f\"✓ Ingested {len(documents)} documents from bioRxiv RSS feed\")\n",
        "print(f\"✓ Normalized {len(normalized_documents)} documents\")\n",
        "print(f\"✓ Deduplicated {len(variants)} variants to {len(deduplicated_variants)} unique variants\")\n",
        "print(f\"✓ Built temporal KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"✓ Detected {len(communities)} communities and {len(pathways)} pathways\")\n",
        "print(f\"✓ This cookbook demonstrates graph analytics, temporal reasoning, and pathway analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
