{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/biomedical/02_Genomic_Variant_Analysis.ipynb)\n",
    "\n",
    "# Genomic Variant Analysis - Graph Analytics & Pathway Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **genomic variant analysis** using Semantica's modular architecture with focus on **graph analytics**, **pathway analysis**, and **temporal knowledge graphs**. The pipeline analyzes genomic data to extract variant entities, build temporal genomic knowledge graphs, and analyze disease associations through reasoning.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Graph Analytics Focus**: Emphasizes graph reasoning, centrality measures, and pathway analysis\n",
    "- **Temporal Analysis**: Builds temporal genomic knowledge graphs to track variant evolution\n",
    "- **Disease Association**: Analyzes relationships between variants, genes, and diseases\n",
    "- **Pathway Analysis**: Uses graph traversal to identify biological pathways\n",
    "- **Impact Prediction**: Predicts variant impact using graph-based reasoning\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- How to use Semantica modules directly for genomic analysis\n",
    "- How to ingest genomic data from multiple sources\n",
    "- How to extract variant, gene, and disease entities\n",
    "- How to build temporal knowledge graphs\n",
    "- How to perform graph analytics (centrality, communities)\n",
    "- How to use temporal queries for variant evolution\n",
    "- How to analyze pathways using reasoning\n",
    "- How to visualize and export genomic knowledge graphs\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Data Ingestion] --> B[Text Processing]\n",
    "    B --> C[Entity Extraction]\n",
    "    C --> D[Relationship Extraction]\n",
    "    D --> E[Deduplication]\n",
    "    E --> F[Temporal KG]\n",
    "    F --> G[Graph Analytics]\n",
    "    F --> H[Temporal Queries]\n",
    "    G --> I[Pathway Analysis]\n",
    "    H --> I\n",
    "    I --> J[Disease Associations]\n",
    "    J --> K[Visualization]\n",
    "```\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "**PubMed RSS Feeds:**\n",
    "- Genetics, Genomics, Variant Analysis, GWAS\n",
    "- Genomic Medicine, Precision Medicine, Pharmacogenomics\n",
    "- Population Genetics, Evolutionary Genomics\n",
    "\n",
    "**Preprint Servers:**\n",
    "- BioRxiv (Genetics, Genomics)\n",
    "- MedRxiv (Genomics, Clinical Genomics)\n",
    "\n",
    "**Journal RSS Feeds:**\n",
    "- Nature Genetics, Genome Research\n",
    "- American Journal of Human Genetics\n",
    "- Human Molecular Genetics\n",
    "\n",
    "**Database Links (for reference):**\n",
    "- [dbSNP](https://www.ncbi.nlm.nih.gov/snp/) - Single Nucleotide Polymorphism database\n",
    "- [ClinVar](https://www.ncbi.nlm.nih.gov/clinvar/) - Clinical significance of variants\n",
    "- [gnomAD](https://gnomad.broadinstitute.org/) - Genome Aggregation Database\n",
    "- [1000 Genomes](https://www.internationalgenome.org/) - Human genetic variation\n",
    "- [Ensembl](https://www.ensembl.org/) - Genome browser and annotation\n",
    "- [UCSC Genome Browser](https://genome.ucsc.edu/) - Genome visualization\n",
    "- [OMIM](https://www.omim.org/) - Online Mendelian Inheritance in Man\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install Semantica and required dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas groq sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Setup\n",
    "\n",
    "Set up environment variables and configuration constants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "TEMPORAL_GRANULARITY = \"day\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Genomic Data from Multiple Sources\n",
    "\n",
    "Ingest data from comprehensive genomic sources including PubMed RSS feeds, preprint servers, and journal feeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import FeedIngestor, FileIngestor\n",
    "import os\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "feed_sources = [\n",
    "    # PubMed RSS Feeds\n",
    "    (\"PubMed - Genetics\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=genetics&limit=15&sort=pub_date\"),\n",
    "    (\"PubMed - Genomics\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=genomics&limit=15&sort=pub_date\"),\n",
    "    (\"PubMed - Variant Analysis\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=variant+analysis&limit=15&sort=pub_date\"),\n",
    "    (\"PubMed - GWAS\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=GWAS&limit=15&sort=pub_date\"),\n",
    "    (\"PubMed - Genomic Medicine\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=genomic+medicine&limit=15&sort=pub_date\"),\n",
    "    (\"PubMed - Precision Medicine\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=precision+medicine&limit=15&sort=pub_date\"),\n",
    "    (\"PubMed - Pharmacogenomics\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=pharmacogenomics&limit=15&sort=pub_date\"),\n",
    "    (\"PubMed - Population Genetics\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=population+genetics&limit=15&sort=pub_date\"),\n",
    "    (\"PubMed - Evolutionary Genomics\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=evolutionary+genomics&limit=15&sort=pub_date\"),\n",
    "    (\"PubMed - Clinical Genomics\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=clinical+genomics&limit=15&sort=pub_date\"),\n",
    "    \n",
    "    # Preprint Servers\n",
    "    (\"BioRxiv - Genetics\", \"https://connect.biorxiv.org/biorxiv_xml.php?subject=genetics\"),\n",
    "    (\"BioRxiv - Genomics\", \"https://connect.biorxiv.org/biorxiv_xml.php?subject=genomics\"),\n",
    "    (\"MedRxiv - Genomics\", \"https://connect.medrxiv.org/medrxiv_xml.php?subject=genomics\"),\n",
    "    (\"MedRxiv - Clinical Genomics\", \"https://connect.medrxiv.org/medrxiv_xml.php?subject=clinical_genomics\"),\n",
    "    \n",
    "    # Journal RSS Feeds\n",
    "    (\"Nature Genetics\", \"https://www.nature.com/ng.rss\"),\n",
    "    (\"Genome Research\", \"https://genome.cshlp.org/rss/current.xml\"),\n",
    "    (\"American Journal of Human Genetics\", \"https://www.cell.com/ajhg.rss\"),\n",
    "    (\"Human Molecular Genetics\", \"https://academic.oup.com/hmg/rss/current\"),\n",
    "]\n",
    "\n",
    "feed_ingestor = FeedIngestor()\n",
    "all_documents = []\n",
    "\n",
    "for feed_name, feed_url in feed_sources:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            feed_data = feed_ingestor.ingest_feed(feed_url, validate=False)\n",
    "        \n",
    "        for item in feed_data.items:\n",
    "            if not item.content:\n",
    "                item.content = item.description or item.title or \"\"\n",
    "            if item.content:\n",
    "                if not hasattr(item, 'metadata'):\n",
    "                    item.metadata = {}\n",
    "                item.metadata['source'] = feed_name\n",
    "                all_documents.append(item)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not all_documents:\n",
    "    variant_data = \"\"\"\n",
    "    Variant rs699 is located in the AGT gene and associated with hypertension.\n",
    "    Variant rs7412 in APOE gene is linked to Alzheimer's disease risk.\n",
    "    BRCA1 variant c.5266dupC increases breast cancer susceptibility.\n",
    "    CFTR variant F508del causes cystic fibrosis.\n",
    "    Variant rs1800566 in NAT2 gene affects drug metabolism.\n",
    "    Variant rs1042713 in ADRB2 gene is associated with asthma response.\n",
    "    TP53 variant R273H is linked to multiple cancer types.\n",
    "    Variant rs1799853 in CYP2C9 gene affects warfarin metabolism.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"data/variants.txt\", \"w\") as f:\n",
    "        f.write(variant_data)\n",
    "    \n",
    "    file_ingestor = FileIngestor()\n",
    "    all_documents = file_ingestor.ingest(\"data/variants.txt\")\n",
    "\n",
    "documents = all_documents\n",
    "print(f\"Ingested {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and Chunking Genomic Documents\n",
    "\n",
    "Clean and normalize text, then split into chunks using entity-aware chunking to preserve variant/gene entity boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.split import TextSplitter\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "splitter = TextSplitter(\n",
    "    method=\"entity_aware\",\n",
    "    ner_method=\"spacy\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "normalized_documents = []\n",
    "for doc in documents:\n",
    "    normalized_text = normalizer.normalize(\n",
    "        doc.content if hasattr(doc, 'content') else str(doc),\n",
    "        clean_html=True,\n",
    "        normalize_entities=True,\n",
    "        remove_extra_whitespace=True,\n",
    "        lowercase=False\n",
    "    )\n",
    "    normalized_documents.append(normalized_text)\n",
    "\n",
    "chunked_documents = []\n",
    "for doc_text in normalized_documents:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            chunks = splitter.split(doc_text)\n",
    "        chunked_documents.extend(chunks)\n",
    "    except Exception:\n",
    "        simple_splitter = TextSplitter(method=\"recursive\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "        chunks = simple_splitter.split(doc_text)\n",
    "        chunked_documents.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import NERExtractor\n",
    "\n",
    "entity_extractor = NERExtractor(\n",
    "    method=\"llm\",\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "all_entities = []\n",
    "for chunk in chunked_documents:\n",
    "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "    try:\n",
    "        entities = entity_extractor.extract_entities(\n",
    "            chunk_text,\n",
    "            entity_types=[\"Variant\", \"Gene\", \"Disease\", \"Pathway\", \"Protein\"]\n",
    "        )\n",
    "        all_entities.extend(entities)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "variants = [e for e in all_entities if e.label == \"Variant\" or \"variant\" in e.label.lower()]\n",
    "genes = [e for e in all_entities if e.label == \"Gene\" or \"gene\" in e.label.lower()]\n",
    "diseases = [e for e in all_entities if e.label == \"Disease\" or \"disease\" in e.label.lower()]\n",
    "\n",
    "print(f\"Extracted {len(variants)} variants, {len(genes)} genes, {len(diseases)} diseases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Genomic Relationships\n",
    "\n",
    "Extract relationships between variants, genes, and diseases to understand genomic associations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import RelationExtractor\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    method=\"llm\",\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "all_relationships = []\n",
    "for chunk in chunked_documents:\n",
    "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "    try:\n",
    "        relationships = relation_extractor.extract_relations(\n",
    "            chunk_text,\n",
    "            entities=all_entities,\n",
    "            relation_types=[\"associated_with\", \"located_in\", \"causes\", \"increases_risk\", \"affects\", \"linked_to\"]\n",
    "        )\n",
    "        all_relationships.extend(relationships)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving Duplicate Variants\n",
    "\n",
    "Detect and merge duplicate variants to ensure data quality and consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.deduplication import DuplicateDetector\n",
    "\n",
    "duplicate_detector = DuplicateDetector(\n",
    "    similarity_threshold=0.85,\n",
    "    method=\"semantic\"\n",
    ")\n",
    "\n",
    "deduplicated_entities = duplicate_detector.detect_duplicates(all_entities)\n",
    "merged_entities = duplicate_detector.merge_duplicates(deduplicated_entities)\n",
    "\n",
    "print(f\"Deduplicated {len(all_entities)} entities to {len(merged_entities)} unique entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Temporal Genomic Knowledge Graph\n",
    "\n",
    "Construct a temporal knowledge graph from extracted entities and relationships to enable time-aware analysis and variant evolution tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "\n",
    "graph_builder = GraphBuilder(\n",
    "    merge_entities=True,\n",
    "    resolve_conflicts=True,\n",
    "    entity_resolution_strategy=\"fuzzy\",\n",
    "    enable_temporal=True,\n",
    "    temporal_granularity=TEMPORAL_GRANULARITY\n",
    ")\n",
    "\n",
    "kg_sources = [{\n",
    "    \"entities\": [{\"text\": e.text, \"type\": e.label, \"confidence\": e.confidence} for e in merged_entities],\n",
    "    \"relationships\": [{\"source\": r.source, \"target\": r.target, \"type\": r.label, \"confidence\": r.confidence} for r in all_relationships]\n",
    "}]\n",
    "\n",
    "kg = graph_builder.build(kg_sources)\n",
    "\n",
    "entities_count = len(kg.get('entities', []))\n",
    "relationships_count = len(kg.get('relationships', []))\n",
    "print(f\"Graph: {entities_count} entities, {relationships_count} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Graph Structure\n",
    "\n",
    "Perform comprehensive graph analytics including centrality measures, community detection, and connectivity analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphAnalyzer, CentralityCalculator, CommunityDetector\n",
    "\n",
    "graph_analyzer = GraphAnalyzer()\n",
    "centrality_calc = CentralityCalculator()\n",
    "community_detector = CommunityDetector()\n",
    "\n",
    "analysis = graph_analyzer.analyze_graph(kg)\n",
    "\n",
    "degree_centrality = centrality_calc.calculate_degree_centrality(kg)\n",
    "betweenness_centrality = centrality_calc.calculate_betweenness_centrality(kg)\n",
    "closeness_centrality = centrality_calc.calculate_closeness_centrality(kg)\n",
    "\n",
    "communities = community_detector.detect_communities(kg, method=\"louvain\")\n",
    "connectivity = graph_analyzer.analyze_connectivity(kg)\n",
    "\n",
    "print(f\"Graph analytics:\")\n",
    "print(f\"  - Communities: {len(communities)}\")\n",
    "print(f\"  - Connected components: {len(connectivity.get('components', []))}\")\n",
    "print(f\"  - Graph density: {analysis.get('density', 0):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Graph Queries\n",
    "\n",
    "Query the temporal knowledge graph at specific time points, analyze temporal evolution, and detect temporal patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import TemporalGraphQuery\n",
    "\n",
    "temporal_query = TemporalGraphQuery(\n",
    "    enable_temporal_reasoning=True,\n",
    "    temporal_granularity=TEMPORAL_GRANULARITY\n",
    ")\n",
    "\n",
    "query_results = temporal_query.query_at_time(\n",
    "    kg,\n",
    "    query={\"type\": \"Variant\"},\n",
    "    at_time=\"2024-01-01\"\n",
    ")\n",
    "\n",
    "evolution = temporal_query.analyze_evolution(kg)\n",
    "temporal_patterns = temporal_query.detect_temporal_patterns(kg, pattern_type=\"sequence\")\n",
    "\n",
    "print(f\"Temporal queries: {len(query_results)} variants at query time\")\n",
    "print(f\"Temporal patterns detected: {len(temporal_patterns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathway Analysis & Reasoning\n",
    "\n",
    "Use graph reasoning to find pathways between variants and diseases, and infer biological pathways through logical reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.reasoning import Reasoner\n",
    "\n",
    "reasoner = Reasoner()\n",
    "\n",
    "pathways = reasoner.find_paths(\n",
    "    kg,\n",
    "    source_type=\"Variant\",\n",
    "    target_type=\"Disease\",\n",
    "    max_hops=3\n",
    ")\n",
    "\n",
    "reasoner.add_rule(\"IF Variant associated_with Gene AND Gene causes Disease THEN Variant increases_risk Disease\")\n",
    "inferred_facts = reasoner.infer_facts(kg)\n",
    "\n",
    "print(f\"Pathway analysis: {len(pathways)} variant-disease pathways found\")\n",
    "print(f\"Inferred facts: {len(inferred_facts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Disease Associations\n",
    "\n",
    "Use graph traversal to find variant-disease associations and calculate association scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_associations = []\n",
    "for variant in variants[:10]:\n",
    "    variant_name = variant.text\n",
    "    paths = graph_analyzer.find_paths(\n",
    "        kg,\n",
    "        source=variant_name,\n",
    "        target_type=\"Disease\",\n",
    "        max_hops=2\n",
    "    )\n",
    "    for path in paths:\n",
    "        if path.get('target_type') == 'Disease':\n",
    "            disease_associations.append({\n",
    "                'variant': variant_name,\n",
    "                'disease': path.get('target'),\n",
    "                'path_length': len(path.get('path', [])),\n",
    "                'confidence': variant.confidence\n",
    "            })\n",
    "\n",
    "disease_associations.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "print(f\"Top disease associations:\")\n",
    "for i, assoc in enumerate(disease_associations[:5], 1):\n",
    "    print(f\"{i}. {assoc['variant']} -> {assoc['disease']} (confidence: {assoc['confidence']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Temporal Knowledge Graph\n",
    "\n",
    "Generate an interactive visualization of the temporal genomic knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "visualizer = KGVisualizer()\n",
    "visualizer.visualize(\n",
    "    kg,\n",
    "    output_path=\"genomic_variant_kg.html\",\n",
    "    layout=\"hierarchical\",\n",
    "    node_size=20\n",
    ")\n",
    "\n",
    "print(\"Visualization saved to genomic_variant_kg.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results\n",
    "\n",
    "Export the temporal knowledge graph to various formats for further analysis or integration with other tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter\n",
    "\n",
    "exporter = GraphExporter()\n",
    "exporter.export(kg, output_path=\"genomic_variant_kg.json\", format=\"json\")\n",
    "exporter.export(kg, output_path=\"genomic_variant_kg.graphml\", format=\"graphml\")\n",
    "\n",
    "print(\"Exported knowledge graph to JSON and GraphML formats\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
