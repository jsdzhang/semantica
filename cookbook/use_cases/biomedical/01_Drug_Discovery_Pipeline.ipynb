{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/biomedical/01_Drug_Discovery_Pipeline.ipynb)\n",
        "\n",
        "# Drug Discovery Pipeline - Vector Similarity Search\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates a **complete drug discovery pipeline** using Semantica that focuses on **vector similarity search** and **interaction prediction**. The pipeline ingests drug and protein data, extracts compound and target entities, builds a drug-target knowledge graph, and performs similarity search to predict drug-target interactions.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Vector-Focused Approach**: Emphasizes embeddings and vector similarity search for drug-target interaction prediction\n",
        "- **Compound-Target Extraction**: Extracts drug compounds, proteins, and targets from biomedical literature\n",
        "- **Similarity Search**: Uses vector embeddings to find similar compounds and predict interactions\n",
        "- **Knowledge Graph Construction**: Builds structured drug-target relationship graphs\n",
        "- **Interaction Prediction**: Predicts potential drug-target interactions using similarity metrics\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "- How to ingest biomedical data (drug databases, protein data, literature)\n",
        "- How to extract compound and target entities from unstructured text\n",
        "- How to generate embeddings for drugs and proteins\n",
        "- How to perform similarity search to find similar compounds\n",
        "- How to build drug-target knowledge graphs\n",
        "- How to predict drug-target interactions using vector similarity\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Biomedical Data Ingestion\n",
        "3. **Phase 2**: Document Parsing & Processing\n",
        "4. **Phase 3**: Entity Extraction (Drugs, Proteins, Targets)\n",
        "5. **Phase 4**: Embedding Generation\n",
        "6. **Phase 5**: Vector Store Population\n",
        "7. **Phase 6**: Similarity Search & Interaction Prediction\n",
        "8. **Phase 7**: Knowledge Graph Construction\n",
        "9. **Phase 8**: Visualization & Export\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n",
        "\n",
        "Install Semantica and required dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "# Install Semantica and required dependencies\n",
        "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Setup & Configuration\n",
        "\n",
        "- Configure Semantica for drug discovery\n",
        "- Focus on vector similarity search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set API keys\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"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\")\n",
        "\n",
        "print(\"Environment configured.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Import and configure Semantica core components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantica core configured.\n"
          ]
        }
      ],
      "source": [
        "from semantica.core import Semantica, ConfigManager\n",
        "\n",
        "# Configure for drug discovery with vector similarity focus\n",
        "config_dict = {\n",
        "    \"project_name\": \"Drug_Discovery_Pipeline\",\n",
        "    \"embedding\": {\n",
        "        \"provider\": \"sentence_transformers\",\n",
        "        \"model\": \"all-MiniLM-L6-v2\"  # 384-dimensional embeddings\n",
        "    },\n",
        "    \"extraction\": {\n",
        "        \"provider\": \"groq\",\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"temperature\": 0.0\n",
        "    },\n",
        "    \"vector_store\": {\n",
        "        \"provider\": \"faiss\",\n",
        "        \"dimension\": 384\n",
        "    },\n",
        "    \"knowledge_graph\": {\n",
        "        \"backend\": \"networkx\",\n",
        "        \"merge_entities\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "\n",
        "print(\"Semantica core configured.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fastembed not available. Install with: pip install fastembed. Using fallback embedding method.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store initialized.\n"
          ]
        }
      ],
      "source": [
        "from semantica.vector_store import VectorStore\n",
        "\n",
        "# Initialize vector store with FAISS backend\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=384)\n",
        "\n",
        "print(\"Vector store initialized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Biomedical Data Ingestion\n",
        "\n",
        "- Ingest biomedical data from PubMed RSS feeds\n",
        "- Use FeedIngestor for data collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory ready.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Create data directory if it doesn't exist\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "print(\"Data directory ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configured 8 RSS feed sources\n"
          ]
        }
      ],
      "source": [
        "from semantica.ingest import FeedIngestor\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "# Multiple RSS feed options for biomedical data\n",
        "feed_urls = [\n",
        "    (\"PubMed - Drug Discovery\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=drug+discovery&limit=10&sort=pub_date&fc=article_type\"),\n",
        "    (\"PubMed - Drug Target Interaction\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=drug+target+interaction&limit=10&sort=pub_date\"),\n",
        "    (\"PubMed - Pharmacokinetics\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=pharmacokinetics&limit=10&sort=pub_date\"),\n",
        "    (\"PubMed - Pharmacodynamics\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=pharmacodynamics&limit=10&sort=pub_date\"),\n",
        "    (\"PubMed - Clinical Trial\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=clinical+trial&limit=10&sort=pub_date\"),\n",
        "    (\"PubMed - Protein Target\", \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=protein+target&limit=10&sort=pub_date\"),\n",
        "    (\"BioRxiv - Pharmacology\", \"https://connect.biorxiv.org/biorxiv_xml.php?subject=pharmacology_and_toxicology\"),\n",
        "    (\"Nature - Drug Discovery\", \"https://www.nature.com/subjects/drug-discovery.rss\")\n",
        "]\n",
        "\n",
        "print(f\"Configured {len(feed_urls)} RSS feed sources\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedIngestor initialized\n"
          ]
        }
      ],
      "source": [
        "# Initialize FeedIngestor\n",
        "feed_ingestor = FeedIngestor()\n",
        "print(\"FeedIngestor initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style='font-family: monospace;'><h4>üß† Semantica - üìä Current Progress</h4><table style='width: 100%; border-collapse: collapse;'><tr><th>Status</th><th>Action</th><th>Module</th><th>Submodule</th><th>File</th><th>Time</th></tr><tr><td>‚úÖ</td><td>Semantica is normalizing</td><td>üîß normalize</td><td>TextNormalizer</td><td>-</td><td>0.00s</td></tr><tr><td>‚úÖ</td><td>Semantica is extracting</td><td>üéØ semantic_extract</td><td>NERExtractor</td><td>-</td><td>0.65s</td></tr><tr><td>‚úÖ</td><td>Semantica is processing</td><td>‚è≥ core</td><td>LifecycleManager</td><td>-</td><td>1.19s</td></tr><tr><td>üîÑ</td><td>Semantica is processing</td><td>‚è≥ core</td><td>Semantica</td><td>-</td><td>0.06s</td></tr><tr><td>‚úÖ</td><td>Semantica is executing</td><td>‚öôÔ∏è pipeline</td><td>PipelineBuilder</td><td>-</td><td>0.03s</td></tr><tr><td>‚úÖ</td><td>Semantica is executing</td><td>‚öôÔ∏è pipeline</td><td>PipelineValidator</td><td>-</td><td>0.01s</td></tr><tr><td>üîÑ</td><td>Semantica is processing</td><td>‚è≥ core</td><td>build_knowledge_base</td><td>chunk_0.txt</td><td>0.01s</td></tr><tr><td>üîÑ</td><td>Semantica is executing</td><td>‚öôÔ∏è pipeline</td><td>ExecutionEngine</td><td>chunk_0.txt</td><td>0.01s</td></tr><tr><td>üîÑ</td><td>Semantica is executing</td><td>‚öôÔ∏è pipeline</td><td>ExecutionEngine</td><td>-</td><td>0.01s</td></tr><tr><td>üîÑ</td><td>Semantica is executing</td><td>‚öôÔ∏è pipeline</td><td>ResourceScheduler</td><td>-</td><td>0.00s</td></tr></table></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Successfully ingested 30 documents from Nature - Drug Discovery\n"
          ]
        }
      ],
      "source": [
        "# Try each feed URL until one succeeds (suppress error messages)\n",
        "documents = None\n",
        "\n",
        "for feed_name, feed_url in feed_urls:\n",
        "    try:\n",
        "        # Suppress stderr to avoid cluttering output with parsing errors\n",
        "        with redirect_stderr(StringIO()):\n",
        "            feed_data = feed_ingestor.ingest_feed(feed_url, validate=False)\n",
        "        \n",
        "        # Convert FeedItem objects to documents\n",
        "        documents = []\n",
        "        for item in feed_data.items:\n",
        "            if not item.content:\n",
        "                item.content = item.description or item.title or \"\"\n",
        "            if item.content:\n",
        "                documents.append(item)\n",
        "        \n",
        "        if documents:\n",
        "            print(f\"‚úì Successfully ingested {len(documents)} documents from {feed_name}\")\n",
        "            break\n",
        "    except Exception:\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fallback to sample data if all RSS feeds failed\n",
        "from semantica.ingest import FileIngestor\n",
        "\n",
        "if not documents:\n",
        "    sample_drug_data = \"\"\"\n",
        "    Aspirin (acetylsalicylic acid) is a medication used to reduce pain, fever, or inflammation. \n",
        "    It targets cyclooxygenase enzymes COX-1 and COX-2. Aspirin is commonly used for cardiovascular protection.\n",
        "    Ibuprofen is a nonsteroidal anti-inflammatory drug (NSAID) that targets COX-1 and COX-2 enzymes.\n",
        "    Metformin is an antidiabetic medication that targets AMP-activated protein kinase (AMPK).\n",
        "    Insulin targets the insulin receptor (INSR) to regulate glucose metabolism.\n",
        "    Warfarin is an anticoagulant that targets vitamin K epoxide reductase complex subunit 1 (VKORC1).\n",
        "    Atorvastatin is a statin medication that targets HMG-CoA reductase.\n",
        "    \"\"\"\n",
        "    \n",
        "    import os\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    with open(\"data/sample_drugs.txt\", \"w\") as f:\n",
        "        f.write(sample_drug_data)\n",
        "    \n",
        "    file_ingestor = FileIngestor()\n",
        "    documents = file_ingestor.ingest(\"data/sample_drugs.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextNormalizer initialized\n"
          ]
        }
      ],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "\n",
        "# Initialize text normalizer\n",
        "normalizer = TextNormalizer()\n",
        "print(\"TextNormalizer initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized 30 documents\n"
          ]
        }
      ],
      "source": [
        "# Normalize all documents\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True,\n",
        "        lowercase=False  # Preserve drug names (case-sensitive)\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preview normalized text\n",
        "if normalized_documents:\n",
        "    print(f\"Sample normalized text (first 200 chars): {normalized_documents[0][:200]}\")\n",
        "else:\n",
        "    print(\"No normalized documents available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Text Normalization & Cleaning\n",
        "\n",
        "- Clean HTML tags and special characters\n",
        "- Normalize entity names\n",
        "- Remove extra whitespace\n",
        "- Preserve drug names (case-sensitive)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Entity-Aware Chunking\n",
        "\n",
        "- Use entity-aware chunking to preserve drug/protein entity boundaries\n",
        "- Essential for GraphRAG operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextSplitter initialized with entity-aware chunking\n"
          ]
        }
      ],
      "source": [
        "from semantica.split import TextSplitter\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "# Initialize TextSplitter with entity-aware chunking\n",
        "# Using 'spacy' instead of 'llm' to avoid API key requirements\n",
        "splitter = TextSplitter(\n",
        "    method=\"entity_aware\",\n",
        "    ner_method=\"spacy\",  # Use spaCy for entity recognition (no API key needed)\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "print(\"TextSplitter initialized with entity-aware chunking\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 30 chunks\n"
          ]
        }
      ],
      "source": [
        "# Chunk normalized documents (suppress error messages)\n",
        "chunked_documents = []\n",
        "for doc_text in normalized_documents:\n",
        "    try:\n",
        "        # Suppress stderr to avoid cluttering output with NER errors\n",
        "        with redirect_stderr(StringIO()):\n",
        "            chunks = splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "    except Exception:\n",
        "        # Fallback to simple recursive splitting if entity-aware fails\n",
        "        simple_splitter = TextSplitter(method=\"recursive\", chunk_size=1000, chunk_overlap=200)\n",
        "        chunks = simple_splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "\n",
        "print(f\"Created {len(chunked_documents)} chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample chunk (first 200 chars): Apoptotic signatures allow early and rapid screening of drug-induced liver injury to accelerate drug discovery\n"
          ]
        }
      ],
      "source": [
        "# Preview sample chunk\n",
        "if chunked_documents:\n",
        "    sample_chunk = chunked_documents[0]\n",
        "    chunk_text = sample_chunk.text if hasattr(sample_chunk, 'text') else str(sample_chunk)\n",
        "    print(f\"Sample chunk (first 200 chars): {chunk_text[:200]}\")\n",
        "else:\n",
        "    print(\"No chunks available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Entity Extraction & Knowledge Graph Construction\n",
        "\n",
        "- Extract drug and protein entities\n",
        "- Build knowledge graph from extracted entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 30 chunk files for entity extraction\n"
          ]
        }
      ],
      "source": [
        "# Convert chunks to text files\n",
        "# build_knowledge_base requires file paths or URLs, not raw text\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data/chunks\", exist_ok=True)\n",
        "chunk_files = []\n",
        "\n",
        "for i, chunk in enumerate(chunked_documents):\n",
        "    # Extract text from chunk object\n",
        "    if hasattr(chunk, 'text'):\n",
        "        chunk_text = chunk.text\n",
        "    elif hasattr(chunk, 'content'):\n",
        "        chunk_text = chunk.content\n",
        "    else:\n",
        "        chunk_text = str(chunk)\n",
        "    \n",
        "    # Save chunk to temporary file\n",
        "    chunk_file = f\"data/chunks/chunk_{i}.txt\"\n",
        "    with open(chunk_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(chunk_text)\n",
        "    chunk_files.append(chunk_file)\n",
        "\n",
        "print(f\"Prepared {len(chunk_files)} chunk files for entity extraction\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build knowledge base with entity extraction\n",
        "result = core.build_knowledge_base(\n",
        "    sources=chunk_files,\n",
        "    custom_entity_types=[\"Drug\", \"Protein\", \"Target\", \"Compound\", \"Enzyme\", \"Receptor\"],\n",
        "    embeddings=True,\n",
        "    graph=True\n",
        ")\n",
        "\n",
        "print(\"Knowledge base built successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract entities from result\n",
        "entities = result[\"entities\"]\n",
        "print(f\"Total entities extracted: {len(entities)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter drugs and proteins\n",
        "drugs = [e for e in entities if e.get(\"type\") == \"Drug\" or \"drug\" in e.get(\"type\", \"\").lower()]\n",
        "proteins = [e for e in entities if e.get(\"type\") == \"Protein\" or \"protein\" in e.get(\"type\", \"\").lower()]\n",
        "\n",
        "print(f\"Extracted {len(drugs)} drugs and {len(proteins)} proteins\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview sample drugs\n",
        "print(\"Sample drugs:\")\n",
        "for d in drugs[:3]:\n",
        "    print(f\"  - {d.get('text', 'Unknown')[:50]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview sample proteins\n",
        "print(\"Sample proteins:\")\n",
        "for p in proteins[:3]:\n",
        "    print(f\"  - {p.get('text', 'Unknown')[:50]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get knowledge graph statistics\n",
        "kg = result[\"knowledge_graph\"]\n",
        "print(f\"Knowledge graph contains:\")\n",
        "print(f\"  - {len(kg.get('entities', []))} entities\")\n",
        "print(f\"  - {len(kg.get('relationships', []))} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Entity-Aware Chunking\n",
        "\n",
        "- Use entity-aware chunking to preserve drug/protein entity boundaries\n",
        "- Essential for GraphRAG operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define GraphRAG query\n",
        "query = \"What drugs target COX enzymes?\"\n",
        "print(f\"Query: {query}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve using GraphRAG (hybrid vector + graph retrieval)\n",
        "results = context.retrieve(\n",
        "    query,\n",
        "    max_results=10,\n",
        "    use_graph=True,  # Enable graph traversal\n",
        "    expand_graph=True,  # Expand graph relationships\n",
        "    include_entities=True,  # Include related entities\n",
        "    include_relationships=True  # Include relationships\n",
        ")\n",
        "\n",
        "print(f\"GraphRAG retrieved {len(results)} results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display GraphRAG results\n",
        "for i, result in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
        "    print(f\"   Content: {result.get('content', '')[:200]}...\")\n",
        "    if result.get('related_entities'):\n",
        "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 30 chunks using entity-aware chunking\n"
          ]
        }
      ],
      "source": [
        "from semantica.split import TextSplitter\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "# Use entity-aware chunking to preserve drug/protein entity boundaries\n",
        "# Using 'spacy' instead of 'llm' to avoid API key requirements\n",
        "splitter = TextSplitter(\n",
        "    method=\"entity_aware\",\n",
        "    ner_method=\"spacy\",  # Use spaCy for entity recognition (no API key needed)\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Chunk normalized documents (suppress error messages)\n",
        "chunked_documents = []\n",
        "for doc_text in normalized_documents:\n",
        "    try:\n",
        "        # Suppress stderr to avoid cluttering output with NER errors\n",
        "        with redirect_stderr(StringIO()):\n",
        "            chunks = splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "    except Exception:\n",
        "        # Fallback to simple recursive splitting if entity-aware fails\n",
        "        simple_splitter = TextSplitter(method=\"recursive\", chunk_size=1000, chunk_overlap=200)\n",
        "        chunks = simple_splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "\n",
        "print(f\"Created {len(chunked_documents)} chunks using entity-aware chunking\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare drug texts for embedding\n",
        "drug_texts = [f\"{d.get('text', '')} {d.get('description', '')}\" for d in drugs]\n",
        "print(f\"Prepared {len(drug_texts)} drug texts for embedding\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate drug embeddings\n",
        "drug_embeddings = embedding_gen.generate_embeddings(drug_texts)\n",
        "print(f\"Generated {len(drug_embeddings)} drug embeddings\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store embeddings in vector store\n",
        "drug_ids = vector_store.store_vectors(\n",
        "    vectors=drug_embeddings,\n",
        "    metadata=[{\"type\": \"drug\", \"name\": d.get(\"text\", \"\")} for d in drugs]\n",
        ")\n",
        "\n",
        "print(f\"Stored {len(drug_ids)} drug embeddings in vector store\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Entity Extraction & Knowledge Graph Construction\n",
        "\n",
        "- Extract drug and protein entities\n",
        "- Build knowledge graph from extracted entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate query embedding\n",
        "query_embedding = embedding_gen.generate_embeddings([query_drug])[0]\n",
        "print(\"Query embedding generated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for similar drugs\n",
        "similar_drugs = vector_store.search_vectors(query_embedding, k=5)\n",
        "print(f\"Found {len(similar_drugs)} similar drugs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display similar drugs\n",
        "print(f\"Drugs similar to '{query_drug}':\")\n",
        "for i, result in enumerate(similar_drugs, 1):\n",
        "    print(f\"{i}. {result['metadata'].get('name', 'Unknown')} (similarity: {result['score']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 30 chunk files for entity extraction\n"
          ]
        }
      ],
      "source": [
        "# Convert chunks to text strings and save to temporary files\n",
        "# build_knowledge_base requires file paths or URLs, not raw text\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data/chunks\", exist_ok=True)\n",
        "chunk_files = []\n",
        "\n",
        "for i, chunk in enumerate(chunked_documents):\n",
        "    # Extract text from chunk object\n",
        "    if hasattr(chunk, 'text'):\n",
        "        chunk_text = chunk.text\n",
        "    elif hasattr(chunk, 'content'):\n",
        "        chunk_text = chunk.content\n",
        "    else:\n",
        "        chunk_text = str(chunk)\n",
        "    \n",
        "    # Save chunk to temporary file\n",
        "    chunk_file = f\"data/chunks/chunk_{i}.txt\"\n",
        "    with open(chunk_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(chunk_text)\n",
        "    chunk_files.append(chunk_file)\n",
        "\n",
        "print(f\"Prepared {len(chunk_files)} chunk files for entity extraction\")\n",
        "\n",
        "# Build knowledge base with entity extraction\n",
        "result = core.build_knowledge_base(\n",
        "    sources=chunk_files,\n",
        "    custom_entity_types=[\"Drug\", \"Protein\", \"Target\", \"Compound\", \"Enzyme\", \"Receptor\"],\n",
        "    embeddings=True,\n",
        "    graph=True\n",
        ")\n",
        "\n",
        "# Extract entities\n",
        "entities = result[\"entities\"]\n",
        "drugs = [e for e in entities if e.get(\"type\") == \"Drug\" or \"drug\" in e.get(\"type\", \"\").lower()]\n",
        "proteins = [e for e in entities if e.get(\"type\") == \"Protein\" or \"protein\" in e.get(\"type\", \"\").lower()]\n",
        "\n",
        "print(f\"Extracted {len(drugs)} drugs and {len(proteins)} proteins\")\n",
        "print(f\"\\nSample drugs:\")\n",
        "for d in drugs[:3]:\n",
        "    print(f\"  - {d.get('text', 'Unknown')[:50]}\")\n",
        "print(f\"\\nSample proteins:\")\n",
        "for p in proteins[:3]:\n",
        "    print(f\"  - {p.get('text', 'Unknown')[:50]}\")\n",
        "\n",
        "# Get knowledge graph\n",
        "kg = result[\"knowledge_graph\"]\n",
        "print(f\"\\nKnowledge graph contains:\")\n",
        "print(f\"  - {len(kg.get('entities', []))} entities\")\n",
        "print(f\"  - {len(kg.get('relationships', []))} relationships\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define GraphRAG query\n",
        "query = \"What drugs target COX enzymes?\"\n",
        "print(f\"Query: {query}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve using GraphRAG (hybrid vector + graph retrieval)\n",
        "results = context.retrieve(\n",
        "    query,\n",
        "    max_results=10,\n",
        "    use_graph=True,  # Enable graph traversal\n",
        "    expand_graph=True,  # Expand graph relationships\n",
        "    include_entities=True,  # Include related entities\n",
        "    include_relationships=True  # Include relationships\n",
        ")\n",
        "\n",
        "print(f\"GraphRAG retrieved {len(results)} results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display GraphRAG results\n",
        "for i, result in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
        "    print(f\"   Content: {result.get('content', '')[:200]}...\")\n",
        "    if result.get('related_entities'):\n",
        "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate knowledge graph visualization\n",
        "visualizer.visualize(\n",
        "    kg,\n",
        "    output_path=\"drug_target_kg.html\",\n",
        "    layout=\"spring\",\n",
        "    node_size=20\n",
        ")\n",
        "\n",
        "print(\"Knowledge graph visualization saved to drug_target_kg.html\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display graph statistics\n",
        "print(f\"Graph contains {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Embedding Generation & Vector Store Population\n",
        "\n",
        "- Generate embeddings for drugs and proteins\n",
        "- Populate vector store for similarity search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.context import AgentContext\n",
        "\n",
        "# Initialize GraphRAG context with vector store and knowledge graph\n",
        "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
        "\n",
        "# Example GraphRAG query: Find drugs and their targets\n",
        "query = \"What drugs target COX enzymes?\"\n",
        "print(f\"Query: {query}\\n\")\n",
        "\n",
        "# Retrieve using GraphRAG (hybrid vector + graph retrieval)\n",
        "results = context.retrieve(\n",
        "    query,\n",
        "    max_results=10,\n",
        "    use_graph=True,  # Enable graph traversal\n",
        "    expand_graph=True,  # Expand graph relationships\n",
        "    include_entities=True,  # Include related entities\n",
        "    include_relationships=True  # Include relationships\n",
        ")\n",
        "\n",
        "print(f\"GraphRAG retrieved {len(results)} results:\\n\")\n",
        "for i, result in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
        "    print(f\"   Content: {result.get('content', '')[:200]}...\")\n",
        "    if result.get('related_entities'):\n",
        "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Similarity Search & Interaction Prediction\n",
        "\n",
        "- Use vector similarity to find similar drugs\n",
        "- Predict drug-target interactions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.embeddings import EmbeddingGenerator\n",
        "\n",
        "# Generate embeddings for drugs and proteins\n",
        "embedding_gen = EmbeddingGenerator(provider=\"sentence_transformers\", model=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create drug embeddings\n",
        "drug_texts = [f\"{d.get('text', '')} {d.get('description', '')}\" for d in drugs]\n",
        "drug_embeddings = embedding_gen.generate_embeddings(drug_texts)\n",
        "\n",
        "# Store in vector store\n",
        "drug_ids = vector_store.store_vectors(\n",
        "    vectors=drug_embeddings,\n",
        "    metadata=[{\"type\": \"drug\", \"name\": d.get(\"text\", \"\")} for d in drugs]\n",
        ")\n",
        "\n",
        "print(f\"Stored {len(drug_ids)} drug embeddings in vector store\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve using GraphRAG (hybrid vector + graph retrieval)\n",
        "results = context.retrieve(\n",
        "    query,\n",
        "    max_results=10,\n",
        "    use_graph=True,  # Enable graph traversal\n",
        "    expand_graph=True,  # Expand graph relationships\n",
        "    include_entities=True,  # Include related entities\n",
        "    include_relationships=True  # Include relationships\n",
        ")\n",
        "\n",
        "print(f\"GraphRAG retrieved {len(results)} results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display GraphRAG results\n",
        "for i, result in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
        "    print(f\"   Content: {result.get('content', '')[:200]}...\")\n",
        "    if result.get('related_entities'):\n",
        "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Knowledge Graph Visualization\n",
        "\n",
        "- Visualize drug-target knowledge graph\n",
        "- Display relationships between entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "# Initialize KG visualizer\n",
        "visualizer = KGVisualizer()\n",
        "print(\"KGVisualizer initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate knowledge graph visualization\n",
        "visualizer.visualize(\n",
        "    kg,\n",
        "    output_path=\"drug_target_kg.html\",\n",
        "    layout=\"spring\",\n",
        "    node_size=20\n",
        ")\n",
        "\n",
        "print(\"Knowledge graph visualization saved to drug_target_kg.html\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display graph statistics\n",
        "print(f\"Graph contains {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Find drugs similar to Aspirin\n",
        "query_drug = \"Aspirin\"\n",
        "query_embedding = embedding_gen.generate_embeddings([query_drug])[0]\n",
        "\n",
        "# Search for similar drugs\n",
        "similar_drugs = vector_store.search_vectors(query_embedding, k=5)\n",
        "\n",
        "print(f\"Drugs similar to '{query_drug}':\")\n",
        "for i, result in enumerate(similar_drugs, 1):\n",
        "    print(f\"{i}. {result['metadata'].get('name', 'Unknown')} (similarity: {result['score']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "# Get knowledge graph from result\n",
        "kg = result[\"knowledge_graph\"]\n",
        "\n",
        "# Visualize drug-target relationships\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(\n",
        "    kg,\n",
        "    output_path=\"drug_target_kg.html\",\n",
        "    layout=\"spring\",\n",
        "    node_size=20\n",
        ")\n",
        "\n",
        "print(\"Knowledge graph visualization saved to drug_target_kg.html\")\n",
        "print(f\"Graph contains {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
