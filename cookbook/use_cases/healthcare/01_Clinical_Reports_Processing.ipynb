{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/healthcare/01_Clinical_Reports_Processing.ipynb)\n",
        "\n",
        "# Clinical Reports Processing - EHR Integration & Triplet Stores\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **clinical reports processing** using Semantica with focus on **EHR integration**, **triplet stores**, and **patient knowledge graphs**. The pipeline processes EHR systems and HL7/FHIR APIs to build patient knowledge graphs and store them in triplet stores.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **EHR Integration**: Processes EHR systems and HL7/FHIR APIs\n",
        "- **Triplet Store Storage**: Stores patient data in RDF triplet stores\n",
        "- **Patient Knowledge Graphs**: Builds comprehensive patient KGs\n",
        "- **Medical Entity Extraction**: Extracts medical entities from clinical reports\n",
        "- **Structured Data Storage**: Emphasizes storage and structured data management\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: EHR/HL7/FHIR Data Ingestion\n",
        "3. **Phase 2**: Clinical Report Parsing\n",
        "4. **Phase 3**: Medical Entity Extraction\n",
        "5. **Phase 4**: Patient Knowledge Graph Construction\n",
        "6. **Phase 5**: Triplet Store Population\n",
        "7. **Phase 6**: Visualization & Export\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq rdflib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.triplet_store import TripletStore\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"Clinical_Reports_Processing\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\"}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "triplet_store = TripletStore(backend=\"jena\")  # or \"blazegraph\"\n",
        "print(\"Configured for clinical reports processing with triplet store focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Real Data Ingestion (FDA RSS Feed & HL7/FHIR Structure)\n",
        "\n",
        "Ingest clinical data from FDA RSS feeds and simulated HL7/FHIR databases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FeedIngestor, DBIngestor, FileIngestor\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Option 1: Ingest from FDA RSS feed (real data source)\n",
        "fda_rss_url = \"https://www.fda.gov/about-fda/contact-fda/stay-informed/rss-feeds/fda-press-releases\"\n",
        "\n",
        "try:\n",
        "    feed_ingestor = FeedIngestor()\n",
        "    feed_documents = feed_ingestor.ingest(fda_rss_url, method=\"rss\")\n",
        "    print(f\"Ingested {len(feed_documents)} documents from FDA RSS feed\")\n",
        "    documents.extend(feed_documents)\n",
        "except Exception as e:\n",
        "    print(f\"FDA RSS feed ingestion failed: {e}\")\n",
        "\n",
        "# Option 2: Simulate HL7/FHIR database ingestion\n",
        "# In production: db_ingestor = DBIngestor()\n",
        "# db_ingestor.connect(\"postgresql://user:pass@localhost/emr_db\")\n",
        "# fhir_documents = db_ingestor.ingest(\"SELECT * FROM patient_records\", method=\"postgresql\")\n",
        "\n",
        "# Fallback: Sample clinical report data\n",
        "if not documents:\n",
        "    clinical_data = \"\"\"\n",
        "    Patient ID: P001, Name: John Doe, DOB: 1980-01-15\n",
        "    Diagnosis: Type 2 Diabetes, Date: 2024-01-10\n",
        "    Treatment: Metformin 500mg twice daily, Started: 2024-01-10\n",
        "    Procedure: Blood glucose test, Date: 2024-01-15, Result: 180 mg/dL\n",
        "    Patient ID: P002, Name: Jane Smith, DOB: 1975-05-20\n",
        "    Diagnosis: Hypertension, Date: 2024-01-12\n",
        "    \"\"\"\n",
        "    with open(\"data/clinical_report.txt\", \"w\") as f:\n",
        "        f.write(clinical_data)\n",
        "    documents = FileIngestor().ingest(\"data/clinical_report.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 2: Text Normalization & Deduplication\n",
        "\n",
        "Normalize medical terms and deduplicate patient records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.deduplication import DuplicateDetector\n",
        "\n",
        "# Normalize medical terms\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Build patient knowledge graph\n",
        "result = core.build_knowledge_base(\n",
        "    sources=normalized_documents,\n",
        "    custom_entity_types=[\"Patient\", \"Diagnosis\", \"Treatment\", \"Procedure\", \"EHR\"],\n",
        "    graph=True\n",
        ")\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "entities = result[\"entities\"]\n",
        "\n",
        "# Deduplicate patient records\n",
        "patients = [e for e in entities if e.get(\"type\") == \"Patient\" or \"patient\" in e.get(\"type\", \"\").lower()]\n",
        "\n",
        "detector = DuplicateDetector()\n",
        "duplicates = detector.detect_duplicates(patients, threshold=0.9)\n",
        "deduplicated_patients = detector.resolve_duplicates(patients, duplicates)\n",
        "\n",
        "print(f\"Built patient KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"Deduplicated: {len(patients)} -> {len(deduplicated_patients)} unique patients\")\n",
        "print(\"Focus: EHR integration, triplet stores, patient KGs, structured data storage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store patient data in triplet store\n",
        "triplets = []\n",
        "for rel in kg.get(\"relationships\", []):\n",
        "    triplets.append({\n",
        "        \"subject\": rel.get(\"source\"),\n",
        "        \"predicate\": rel.get(\"predicate\"),\n",
        "        \"object\": rel.get(\"target\")\n",
        "    })\n",
        "\n",
        "triplet_store.store_triplets(triplets)\n",
        "print(f\"Stored {len(triplets)} triplets in triplet store\")\n",
        "print(\"\\n=== Pipeline Summary ===\")\n",
        "print(f\"✓ Ingested {len(documents)} documents from FDA RSS feed and HL7/FHIR\")\n",
        "print(f\"✓ Normalized {len(normalized_documents)} documents\")\n",
        "print(f\"✓ Deduplicated {len(patients)} patients to {len(deduplicated_patients)} unique\")\n",
        "print(f\"✓ Built patient KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"✓ Stored {len(triplets)} triplets in triplet store\")\n",
        "print(f\"✓ This cookbook emphasizes triplet store storage and structured data management\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 6: Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"patient_kg.html\")\n",
        "\n",
        "print(\"Clinical reports processing complete\")\n",
        "print(\"Emphasizes: EHR integration, triplet stores, patient KGs, structured data\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
