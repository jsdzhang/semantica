{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0a7591",
   "metadata": {},
   "source": [
    "# RAG vs. GraphRAG: Final Answer Comparison\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/advanced_rag/02_RAG_vs_GraphRAG_Comparison.ipynb)\n",
    "\n",
    "This notebook demonstrates the difference in final answers generated by **Standard Vector RAG** and **Semantica GraphRAG** for a complex multi-hop query. We use **FalkorDB** as our high-performance graph backend.\n",
    "\n",
    "### FalkorDB Setup\n",
    "To use the persistent graph store, you can start a FalkorDB instance using the provided Docker Compose file:\n",
    "```bash\n",
    "docker compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca782326",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu falkordb sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e971d",
   "metadata": {},
   "source": [
    "## 1. Environment & Store Initialization\n",
    "We set up our **Vector Store** using **Sentence-Transformers** for local embeddings and **FalkorDB** for persistent graph storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_stores",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.core import Semantica, ConfigManager\n",
    "from semantica.vector_store import VectorStore\n",
    "from semantica.graph_store import GraphStore\n",
    "from semantica.kg import GraphBuilder\n",
    "from semantica.split import TextSplitter\n",
    "from semantica.normalize import TextNormalizer\n",
    "import os\n",
    "\n",
    "# 1. Core Semantica & Vector Store\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "config_dict = {\n",
    "    \"embedding\": {\"provider\": \"sentence_transformers\", \"model\": \"all-MiniLM-L6-v2\"},\n",
    "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
    "    \"inference\": {\"provider\": \"groq\", \"model\": \"llama-3.1-70b-versatile\"}\n",
    "}\n",
    "config = ConfigManager().load_from_dict(config_dict)\n",
    "v_core = Semantica(config=config)\n",
    "vs = VectorStore(backend=\"faiss\", dimension=384)\n",
    "\n",
    "# 2. FalkorDB Persistent Graph Store\n",
    "graph_store = GraphStore(\n",
    "    backend=\"falkordb\",\n",
    "    host=\"localhost\",\n",
    "    port=6379,\n",
    "    graph_name=\"intelligence_graph\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    graph_store.connect()\n",
    "    print(\"Successfully connected to FalkorDB backend.\")\n",
    "    use_persistent = True\n",
    "except Exception as e:\n",
    "    print(f\"FalkorDB connection failed: {e}. Falling back to in-memory mode.\")\n",
    "    use_persistent = False\n",
    "\n",
    "# 3. Graph Builder with Persistence Support\n",
    "gb = GraphBuilder(merge_entities=True, resolve_conflicts=True, graph_store=graph_store if use_persistent else None)\n",
    "splitter = TextSplitter(method=\"recursive\", chunk_size=800, chunk_overlap=100)\n",
    "normalizer = TextNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ingestion_section",
   "metadata": {},
   "source": [
    "## 2. Real-Time Intelligence Ingestion\n",
    "We pull live data from global news feeds to build our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ingest_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import FeedIngestor\n",
    "\n",
    "all_content = []\n",
    "feeds = [\n",
    "    \"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
    "    \"https://www.aljazeera.com/xml/rss/all.xml\"\n",
    "]\n",
    "\n",
    "feed_ingestor = FeedIngestor()\n",
    "for f in feeds:\n",
    "    try:\n",
    "        data = feed_ingestor.ingest_feed(f)\n",
    "        for item in data.items[:15]:\n",
    "            text = item.content or item.description or item.title\n",
    "            if text: all_content.append(text)\n",
    "    except Exception: continue\n",
    "\n",
    "clean_docs = [normalizer.normalize(text) for text in all_content if len(text) > 100]\n",
    "chunks = []\n",
    "for doc in clean_docs:\n",
    "    chunks.extend(splitter.split(doc))\n",
    "\n",
    "# Populate Stores\n",
    "print(f\"Processing {len(chunks)} chunks...\")\n",
    "embeddings = v_core.embedding_generator.generate_embeddings([str(c) for c in chunks])\n",
    "vs.store_vectors(vectors=embeddings, metadata=[{\"text\": str(c)} for c in chunks])\n",
    "\n",
    "# Build Knowledge Graph (Automatically persists to FalkorDB if connected)\n",
    "kg = gb.build(sources=[{\"text\": str(c)} for c in chunks[:30]])\n",
    "\n",
    "print(f\"\\nSetup complete. Data ingested and Knowledge Graph built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refinement_intro",
   "metadata": {},
   "source": [
    "## 3. Knowledge Refinement\n",
    "Deduplicating entities and resolving conflicting information for higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refinement_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.deduplication import DuplicateDetector, EntityMerger\n",
    "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
    "\n",
    "# 1. Entity Resolution\n",
    "detector = DuplicateDetector(threshold=0.85)\n",
    "merger = EntityMerger()\n",
    "duplicates = detector.detect_duplicates(kg['entities'])\n",
    "kg_refined = merger.merge_entities(kg, duplicates)\n",
    "\n",
    "# 2. Conflict Detection\n",
    "conflict_detector = ConflictDetector()\n",
    "conflicts = conflict_detector.detect_value_conflicts(kg_refined['entities'], property_name=\"name\")\n",
    "if conflicts:\n",
    "    print(f\"Detected {len(conflicts)} property conflicts.\")\n",
    "    resolver = ConflictResolver()\n",
    "    results = resolver.resolve_conflicts(conflicts, strategy=\"voting\")\n",
    "    print(f\"Successfully resolved {len(results)} conflicts.\")\n",
    "\n",
    "print(\"Knowledge Graph Refined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag_section",
   "metadata": {},
   "source": [
    "## 4. Standard Vector RAG Answer\n",
    "Standard RAG retrieves isolated chunks based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vec = v_core.embedding_generator.generate_embeddings(query)\n",
    "v_res = vs.search_vectors(q_vec, k=1)\n",
    "\n",
    "print(\"--- VECTOR RAG FINAL ANSWER ---\")\n",
    "if v_res:\n",
    "    print(v_res[0]['metadata']['text'])\n",
    "else:\n",
    "    print(\"No relevant context found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphrag_section",
   "metadata": {},
   "source": [
    "## 5. Semantica GraphRAG Answer\n",
    "GraphRAG uses multi-hop traversal via **FalkorDB** to synthesize a connected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphrag_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import ContextRetriever\n",
    "\n",
    "# Use the persistent GraphStore if available, otherwise fallback to the in-memory dict\n",
    "kg_backend = graph_store if use_persistent else kg_refined\n",
    "\n",
    "retriever = ContextRetriever(\n",
    "    vector_store=vs, \n",
    "    knowledge_graph=kg_backend, \n",
    "    use_graph_expansion=True,\n",
    "    max_expansion_hops=2\n",
    ")\n",
    "\n",
    "g_res = retriever.retrieve(query, max_results=1)\n",
    "\n",
    "print(\"--- GRAPHRAG FINAL ANSWER ---\")\n",
    "if g_res:\n",
    "    print(g_res[0].content)\n",
    "    if g_res[0].related_entities:\n",
    "        print(\"\\n--- CONNECTED ENTITIES DISCOVERED ---\")\n",
    "        for ent in g_res[0].related_entities[:5]:\n",
    "            print(f\"- {ent['content']} ({ent['type']}) via {ent['relationship']}\")\n",
    "else:\n",
    "    print(\"No relevant multi-hop context found.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
