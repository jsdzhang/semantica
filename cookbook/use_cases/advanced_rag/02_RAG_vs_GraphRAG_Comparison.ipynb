{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG vs. GraphRAG: \n",
                "\n",
                "## Overview\n",
                "\n",
                "This notebook provides a rigorous, side-by-side comparison of Standard RAG (Vector-based) and GraphRAG (Graph-based) focusing on the Global Intelligence & security domain.\n",
                "\n",
                "### The Challenge: Navigating Fragmentation\n",
                "In intelligence work, facts are scattered. One report might mention a person, another a location, and a third a specific project. Vector search often fails to bridge these \"semantic gaps\" if the keywords aren't directly co-located.\n",
                "\n",
                "We will demonstrate how GraphRAG creates a \"Chain of Evidence\" that Vector RAG simply cannot see.\n",
                "\n",
                "### Key Semantica Modules Utilized\n",
                "\n",
                "| Pipeline Stage | Modules Selection |\n",
                "| :--- | :--- |\n",
                "| **Intelligence Gathering** | `semantica.ingest`, `semantica.normalize` |\n",
                "| **Vector Pipeline** | `semantica.split`, `semantica.vector_store` |\n",
                "| **Graph Pipeline** | `semantica.kg`, `semantica.deduplication`, `semantica.conflicts` |\n",
                "| **Inference/Reasoning**| `semantica.reasoning`, `semantica.pipeline` |\n",
                "| **Interface** | `semantica.context`, `semantica.visualization` |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "!pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Domain Acquisition: Real-World Intelligence Feeds\n",
                "\n",
                "We ingest from real-world feeds to build our knowledge base. We'll look for connections across global security news and official reports."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.ingest import WebIngestor, FeedIngestor\n",
                "from semantica.normalize import TextNormalizer\n",
                "\n",
                "normalizer = TextNormalizer()\n",
                "all_content = []\n",
                "\n",
                "print(\"Gathering Intelligence Data...\")\n",
                "\n",
                "# 1. Global News Feeds\n",
                "feeds = [\n",
                "    \"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
                "    \"https://www.reutersagency.com/feed/\" \n",
                "]\n",
                "feed_ingestor = FeedIngestor()\n",
                "for f in feeds:\n",
                "    docs = feed_ingestor.ingest(f)[:5]\n",
                "    all_content.extend([d.content if hasattr(d, 'content') else str(d) for d in docs])\n",
                "\n",
                "# 2. Official Intelligence/Security Overviews\n",
                "web_urls = [\n",
                "    \"https://www.cia.gov/the-world-factbook/\",\n",
                "    \"https://www.un.org/en/observances/security-council-day\"\n",
                "]\n",
                "web_ingestor = WebIngestor()\n",
                "for url in web_urls:\n",
                "    docs = web_ingestor.ingest(url, method=\"url\")\n",
                "    all_content.extend([d.content if hasattr(d, 'content') else str(d) for d in docs])\n",
                "\n",
                "clean_docs = [normalizer.normalize(text) for text in all_content if len(text) > 100]\n",
                "\n",
                "print(f\"\\nIntelligence Knowledge Hub Populated with {len(clean_docs)} reports.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Standard Vector RAG Pipeline\n",
                "\n",
                "The baseline approach: Linear retrieval via semantic overlap."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.core import Semantica, ConfigManager\n",
                "from semantica.split import TextSplitter\n",
                "from semantica.vector_store import VectorStore\n",
                "\n",
                "v_core = Semantica(config=ConfigManager().load_from_dict({\n",
                "    \"embedding\": {\"provider\": \"openai\", \"model\": \"text-embedding-3-small\"},\n",
                "    \"vector_store\": {\"provider\": \"faiss\", \"dimension\": 1536}\n",
                "}))\n",
                "\n",
                "splitter = TextSplitter(method=\"recursive\", chunk_size=800, chunk_overlap=100)\n",
                "chunks = []\n",
                "for doc in clean_docs:\n",
                "    chunks.extend(splitter.split(doc))\n",
                "\n",
                "vs = VectorStore(backend=\"faiss\", dimension=1536)\n",
                "embeddings = v_core.embedding_generator.generate_embeddings([str(c) for c in chunks[:20]])\n",
                "vs.store_vectors(vectors=embeddings, metadata=[{\"text\": str(c)} for c in chunks[:20]])\n",
                "\n",
                "print(f\"Vector RAG ready with {len(chunks[:20])} encoded fragments.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. High-Fidelity GraphRAG Pipeline\n",
                "\n",
                "Utilizing Entity resolution and relationship synthesis to bridge reports."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.kg import GraphBuilder\n",
                "from semantica.deduplication import DuplicateDetector\n",
                "from semantica.conflicts import ConflictDetector\n",
                "\n",
                "gb = GraphBuilder(merge_entities=True)\n",
                "kg = gb.build(sources=[{\"text\": text} for text in clean_docs[:10]])\n",
                "\n",
                "detector = DuplicateDetector(similarity_threshold=0.85)\n",
                "\n",
                "print(f\"GraphRAG Synthesis Complete: {kg.number_of_nodes()} Entity Nodes mapped.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. The Intelligence Test: Multi-Source Linkage\n",
                "\n",
                "Intelligence query: \"What are the current global security challenges mentioned across different regions?\"\n",
                "\n",
                "Vector RAG will likely return fragments about specific countries but fail to group them. GraphRAG will traverse nodes of type Region to find shared CHALLENGE edges."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.reasoning import GraphReasoner\n",
                "\n",
                "query = \"Identify interconnected security risks across the UN and major regions.\"\n",
                "print(f\"Investigative Query: {query}\\n\")\n",
                "\n",
                "print(\"--- Standard Vector Recall ---\")\n",
                "q_vec = v_core.embedding_generator.generate_embeddings(query)\n",
                "v_res = vs.search_vectors(q_vec, k=3)\n",
                "for r in v_res:\n",
                "    print(f\"Recall: {r['metadata']['text'][:150]}...\")\n",
                "\n",
                "print(\"\\n--- Graph Intelligence Reasoning ---\")\n",
                "reasoner = GraphReasoner(graph=kg)\n",
                "g_res = reasoner.reason(query, depth=2)\n",
                "print(f\"Final Combined Intelligence: {g_res[:400]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualizing the Semantic Network\n",
                "\n",
                "We visualize how the Semantica engine has mapped the relationships between global actors and current events."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.visualization import KGVisualizer\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "KGVisualizer().visualize_network(\n",
                "    kg, \n",
                "    layout=\"spring\", \n",
                "    output=\"static\",\n",
                "    title=\"Intelligence Connectivity Map\"\n",
                ")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
