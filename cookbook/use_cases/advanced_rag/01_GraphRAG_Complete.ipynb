{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Ultimate End-to-End GraphRAG Pipeline\n",
                "\n",
                "## Overview\n",
                "\n",
                "This notebook is the definitive guide to building high-performance, production-ready Knowledge Graph systems using the Semantica framework. We go beyond simple retrieval to demonstrate a full orchestration of the library's advanced capabilities.\n",
                "\n",
                "### What We Are Building\n",
                "\n",
                "We will develop a Self-Evolving Knowledge Base for \"Python Ecosystem Intelligence.\" This system will aggregate verified facts, real-time news, and technical documentation into a queryable, 3D-visualizable graph.\n",
                "\n",
                "### Modules Covered\n",
                "\n",
                "| Module | Purpose |\n",
                "| :--- | :--- |\n",
                "| **`semantica.core`** | Central orchestration and configuration management. |\n",
                "| **`semantica.seed`** | Bootstrapping the graph with verified \"Ground Truth\" data. |\n",
                "| **`semantica.ingest`** | Fetching data from Web, RSS, and Git repositories. |\n",
                "| **`semantica.parse`** | Deep extraction from PDFs, Markdown, and HTML. |\n",
                "| **`semantica.normalize`** | standardizing text, symbols, and entities. |\n",
                "| **`semantica.split`** | Semantic chunking to preserve relationship integrity. |\n",
                "| **`semantica.kg`** | LLM-driven Graph Construction and Analytics. |\n",
                "| **`semantica.deduplication`** | Merging duplicate entities across sources. |\n",
                "| **`semantica.conflicts`** | Resolving discrepancies between sources (e.g., conflicting dates). |\n",
                "| **`semantica.vector_store`** | High-dimensional semantic indexing. |\n",
                "| **`semantica.reasoning`** | Multi-hop graph inference and logic. |\n",
                "| **`semantica.pipeline`** | Wrapping the entire workflow into a repeatable object. |\n",
                "| **`semantica.visualization`** | Rich network graphs and community insights. |\n",
                "| **`semantica.export`** | Persistence to JSON, CSV, and Neo4j. |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Ignoring invalid distribution ~gno (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ython-socketio (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~gno (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ython-socketio (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~gno (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ython-socketio (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
                        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
                        "descript-audiotools 0.7.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.8 which is incompatible.\n",
                        "langchain-openai 0.2.10 requires langchain-core<0.4.0,>=0.3.21, but you have langchain-core 0.1.23 which is incompatible.\n",
                        "mistral-common 1.5.1 requires tiktoken<0.8.0,>=0.7.0, but you have tiktoken 0.12.0 which is incompatible.\n",
                        "nari-tts 0.1.0 requires numpy>=2.2.4, but you have numpy 1.26.4 which is incompatible.\n",
                        "nari-tts 0.1.0 requires torch>=2.6.0, but you have torch 2.2.1 which is incompatible.\n",
                        "parlant 3.0.2 requires fastapi==0.115.12, but you have fastapi 0.120.4 which is incompatible.\n",
                        "parlant 3.0.2 requires fastmcp==2.6.1, but you have fastmcp 2.14.1 which is incompatible.\n",
                        "parlant 3.0.2 requires opentelemetry-exporter-otlp-proto-grpc==1.27.0, but you have opentelemetry-exporter-otlp-proto-grpc 1.38.0 which is incompatible.\n",
                        "parlant 3.0.2 requires rich<14.0.0,>=13.7.1, but you have rich 14.2.0 which is incompatible.\n",
                        "parlant 3.0.2 requires tiktoken<0.9,>=0.8, but you have tiktoken 0.12.0 which is incompatible.\n",
                        "parlant 3.0.2 requires tokenizers<0.21,>=0.20, but you have tokenizers 0.21.4 which is incompatible.\n",
                        "parlant 3.0.2 requires uvicorn<0.33.0,>=0.32.1, but you have uvicorn 0.38.0 which is incompatible.\n",
                        "vllm 0.6.6.post1 requires torch==2.5.1, but you have torch 2.2.1 which is incompatible.\n",
                        "vllm 0.6.6.post1 requires torchvision==0.20.1, but you have torchvision 0.17.1 which is incompatible.\n"
                    ]
                }
            ],
            "source": [
                "# Environment Setup\n",
                "!pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu tiktoken beautifulsoup4 python-docx pdfplumber"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Professional Initialization & Config\n",
                "\n",
                "We start by defining a production config. Semantica uses ConfigManager to ensure environment consistency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Config Loaded.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from semantica.core import Semantica, ConfigManager\n",
                "\n",
                "# Enterprise Config Definition\n",
                "config_dict = {\n",
                "    \"project_name\": \"PythonAI_Mastery\",\n",
                "    \"embedding\": {\n",
                "        \"provider\": \"openai\",\n",
                "        \"model\": \"text-embedding-3-small\"\n",
                "    },\n",
                "    \"extraction\": {\n",
                "        \"model\": \"gpt-4o-mini\",\n",
                "        \"temperature\": 0.0\n",
                "    },\n",
                "    \"vector_store\": {\n",
                "        \"provider\": \"faiss\",\n",
                "        \"dimension\": 1536 \n",
                "    },\n",
                "    \"knowledge_graph\": {\n",
                "        \"backend\": \"networkx\",\n",
                "        \"merge_entities\": True,\n",
                "        \"resolution_strategy\": \"fuzzy\"\n",
                "    }\n",
                "}\n",
                "\n",
                "config = ConfigManager().load_from_dict(config_dict)\n",
                "core = Semantica(config=config)\n",
                "print(\"Config Loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Bootstrapping with Seed Data\n",
                "\n",
                "We use `semantica.seed` to establish \"Ground Truth.\" This prevents the system from being solely dependent on AI extractions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div style='font-family: monospace;'><h4>ðŸ§  Semantica - ðŸ“Š Current Progress</h4><table style='width: 100%; border-collapse: collapse;'><tr><th>Status</th><th>Action</th><th>Module</th><th>Submodule</th><th>File</th><th>Time</th></tr><tr><td>âœ…</td><td>Semantica is seeding</td><td>ðŸŒ± seed</td><td>SeedDataManager</td><td>-</td><td>0.05s</td></tr></table></div>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Foundation Graph Seeded with 2 Verified Nodes.\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "from semantica.seed import SeedDataManager\n",
                "\n",
                "# Create sample ground truth entities\n",
                "foundation_data = {\n",
                "    \"entities\": [\n",
                "        {\"id\": \"python_org\", \"name\": \"Python Software Foundation\", \"type\": \"Organization\"},\n",
                "        {\"id\": \"guido_van_rossum\", \"name\": \"Guido van Rossum\", \"type\": \"Person\"}\n",
                "    ],\n",
                "    \"relationships\": [\n",
                "        {\"source\": \"guido_van_rossum\", \"target\": \"python_org\", \"type\": \"FOUNDED\"}\n",
                "    ]\n",
                "}\n",
                "\n",
                "with open(\"ground_truth.json\", \"w\") as f:\n",
                "    json.dump(foundation_data, f)\n",
                "\n",
                "seed_manager = SeedDataManager()\n",
                "seed_manager.register_source(\"core_info\", \"json\", \"ground_truth.json\")\n",
                "foundation_graph = seed_manager.create_foundation_graph()\n",
                "\n",
                "print(f\"Foundation Graph Seeded with {len(foundation_data['entities'])} Verified Nodes.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Knowledge Hub: Massive Multi-Source Ingestion\n",
                "\n",
                "We aggregate data from a diverse set of real-world sources using `semantica.ingest` and `semantica.parse`. \n",
                "\n",
                "### Data Sources\n",
                "*   **Official Docs**: Python.org, SQLAlchemy, Pydantic.\n",
                "*   **Live News (RSS)**: TechCrunch, Wired, Ars Technica.\n",
                "*   **Technical Blogs**: Real Python, Toward Data Science.\n",
                "*   **Engineering Repos**: Requests, HTTPX, Semantica."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ingesting Official Documentation...\n",
                        "\n",
                        "Fetching Live Tech News...\n",
                        "\n",
                        "Ingesting Engineering READMEs...\n",
                        "\n",
                        "Aggregated 14 documents from across the web.\n"
                    ]
                }
            ],
            "source": [
                "from semantica.ingest import ingest_web, ingest_feed\n",
                "from semantica.parse import parse_document\n",
                "\n",
                "all_content = []\n",
                "\n",
                "# 1. Web Domain Ingestion\n",
                "print(\"Ingesting Official Documentation...\")\n",
                "web_urls = [\n",
                "    \"https://www.python.org/about/\",\n",
                "    \"https://www.python.org/downloads/\",\n",
                "    \"https://realpython.com/\"  # Fixed 404: updated from /python-news/\n",
                "]\n",
                "\n",
                "for url in web_urls:\n",
                "    try:\n",
                "        # Returns a WebContent object\n",
                "        doc = ingest_web(url, method=\"url\")\n",
                "        all_content.append(doc.text)\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to ingest {url}: {e}\")\n",
                "\n",
                "# 2. Live RSS Feeds\n",
                "print(\"\\nFetching Live Tech News...\")\n",
                "rss_feeds = [\n",
                "    \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
                "    \"https://techcrunch.com/feed/\",\n",
                "    \"https://www.wired.com/feed/rss\"\n",
                "]\n",
                "\n",
                "for feed in rss_feeds:\n",
                "    try:\n",
                "        # Returns a FeedData object\n",
                "        feed_data = ingest_feed(feed, method=\"rss\")\n",
                "        # Extract top 3 items from each feed\n",
                "        for item in feed_data.items[:3]:\n",
                "            content = item.content if item.content else item.description\n",
                "            all_content.append(content)\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to ingest feed {feed}: {e}\")\n",
                "\n",
                "# 3. Repository & Technical Files\n",
                "print(\"\\nIngesting Engineering READMEs...\")\n",
                "repo_files = [\n",
                "    \"https://raw.githubusercontent.com/psf/requests/main/README.md\",\n",
                "    \"https://raw.githubusercontent.com/encode/httpx/master/README.md\"\n",
                "]\n",
                "\n",
                "for file_url in repo_files:\n",
                "    try:\n",
                "        # Using ingest_web directly to ensure we get a WebContent object \n",
                "        # (avoiding the dictionary wrapper returned by the unified 'ingest' function)\n",
                "        doc = ingest_web(file_url, method=\"url\") \n",
                "        all_content.append(doc.text)\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to ingest {file_url}: {e}\")\n",
                "\n",
                "print(f\"\\nAggregated {len(all_content)} documents from across the web.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Normalization & Splitting\n",
                "\n",
                "Standardizing noise and chunking for context preservation via `semantica.normalize` and `semantica.split`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
                        "  from tqdm.autonotebook import tqdm, trange\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Normalized text and generated 52 semantic chunks.\n"
                    ]
                }
            ],
            "source": [
                "from semantica.normalize import TextNormalizer\n",
                "from semantica.split import GraphBasedChunker\n",
                "\n",
                "# Normalization - Sanitizing input data\n",
                "normalizer = TextNormalizer()\n",
                "clean_data = [normalizer.normalize(text) for text in all_content if text]\n",
                "\n",
                "# Intelligent Splitting - Preserving semantic boundaries\n",
                "splitter = TextSplitter(\n",
                "    method=\"recursive\", \n",
                "    chunk_size=1200, \n",
                "    chunk_overlap=250\n",
                ")\n",
                "\n",
                "all_chunks = []\n",
                "for doc in clean_data:\n",
                "    all_chunks.extend(splitter.split(doc))\n",
                "\n",
                "print(f\"Normalized text and generated {len(all_chunks)} semantic chunks.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Knowledge Graph Construction & Data Quality\n",
                "\n",
                "Building the graph, then applying Conflict Resolution and Deduplication to ensure data integrity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "ename": "AttributeError",
                    "evalue": "'dict' object has no attribute 'nodes'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Quality Control: Deduplication\u001b[39;00m\n\u001b[0;32m     10\u001b[0m detector \u001b[38;5;241m=\u001b[39m DuplicateDetector(similarity_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m duplicates \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetect_duplicates(\u001b[38;5;28mlist\u001b[39m(\u001b[43mkg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m duplicates:\n\u001b[0;32m     13\u001b[0m     merger \u001b[38;5;241m=\u001b[39m EntityMerger()\n",
                        "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'nodes'"
                    ]
                }
            ],
            "source": [
                "from semantica.kg import GraphBuilder\n",
                "from semantica.deduplication import DuplicateDetector, EntityMerger\n",
                "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
                "\n",
                "# 1. Initial Construction\n",
                "gb = GraphBuilder(merge_entities=True)\n",
                "kg = gb.build(sources=[{\"text\": str(c)} for c in all_chunks[:12]])\n",
                "\n",
                "# 2. Quality Control: Deduplication\n",
                "detector = DuplicateDetector(similarity_threshold=0.85)\n",
                "duplicates = detector.detect_duplicates(list(kg.nodes(data=True)))\n",
                "if duplicates:\n",
                "    merger = EntityMerger()\n",
                "    kg = merger.merge_duplicates(kg, duplicates)\n",
                "    print(\"Deduplicated Entities.\")\n",
                "\n",
                "# 3. Quality Control: Conflict Resolution\n",
                "conflict_detector = ConflictDetector()\n",
                "conflicts = conflict_detector.detect_conflicts(kg)\n",
                "if conflicts:\n",
                "    resolver = ConflictResolver()\n",
                "    kg = resolver.resolve_conflicts(kg, conflicts, strategy=\"most_recent\")\n",
                "    print(\"Resolved Data Conflicts.\")\n",
                "\n",
                "print(f\"High-Quality Knowledge Graph Ready. Nodes: {kg.number_of_nodes()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Graph Synthesis & Advanced Reasoning\n",
                "\n",
                "We apply Graph Analytics and the Reasoning module to derive insights not explicitly stated in the text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.kg import CentralityCalculator, CommunityDetector\n",
                "from semantica.reasoning import GraphReasoner\n",
                "\n",
                "# Analytics - Mapping the Influence\n",
                "centrality = CentralityCalculator().calculate_degree_centrality(kg)\n",
                "communities = CommunityDetector().detect_communities(kg, algorithm=\"louvain\")\n",
                "\n",
                "# GraphRAG Multi-Hop Reasoning - Complex Inference\n",
                "reasoner = GraphReasoner(graph=kg)\n",
                "inference = reasoner.reason(\"What is the impact of Python's latest trends on web development frameworks?\", depth=2)\n",
                "\n",
                "print(f\"Reasoning Agent Insight: {inference[:250]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Hybrid Context Retrieval\n",
                "\n",
                "Storage using `vector_store` and wrapping it in `AgentContext`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.vector_store import VectorStore\n",
                "from semantica.context import AgentContext\n",
                "\n",
                "vs = VectorStore(backend=\"faiss\", dimension=1536)\n",
                "embeddings = core.embedding_generator.generate_embeddings([str(c) for c in all_chunks[:12]])\n",
                "vs.store_vectors(vectors=embeddings, metadata=[{\"text\": str(c)} for c in all_chunks[:12]])\n",
                "\n",
                "# Global Context Manager for an Agent\n",
                "context = AgentContext(vector_store=vs, knowledge_graph=kg)\n",
                "\n",
                "print(\"Hybrid Context Store Initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Immersive Visualization\n",
                "\n",
                "We use `semantica.visualization` to create a community-aware network map."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.visualization import KGVisualizer\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "viz = KGVisualizer()\n",
                "viz.visualize_network(\n",
                "    kg, \n",
                "    layout=\"spring\", \n",
                "    output=\"static\",\n",
                "    title=\"Python Ecosystem Intelligence Graph (Multi-Source)\"\n",
                ")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Modular Orchestration: The Pipeline\n",
                "\n",
                "Finally, we show how to wrap this whole complex flow into a single `semantica.pipeline.Pipeline` object for automation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.pipeline import PipelineBuilder\n",
                "\n",
                "builder = PipelineBuilder()\n",
                "knowledge_pipeline = (\n",
                "    builder.add_step(\"ingest\", \"knowledge_hub_loader\")\n",
                "           .add_step(\"normalize\", \"text_normalizer\")\n",
                "           .add_step(\"split\", \"semantic_splitter\")\n",
                "           .add_step(\"enrich\", \"kg_builder\")\n",
                "           .add_step(\"validate\", \"quality_assurance\")\n",
                "           .build()\n",
                ")\n",
                "\n",
                "print(\"Unified Knowledge Pipeline Construct Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Persistence & Export\n",
                "\n",
                "Save the finalized knowledge structures."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.export import GraphExporter\n",
                "\n",
                "exporter = GraphExporter()\n",
                "exporter.export_to_json(kg, \"master_ecosystem_graph.json\")\n",
                "\n",
                "print(\"Project Exported. Deployment Ready.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
