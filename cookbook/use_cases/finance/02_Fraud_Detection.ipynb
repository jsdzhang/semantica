{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/finance/02_Fraud_Detection.ipynb)\n",
        "\n",
        "# Fraud Detection - Temporal KGs & Pattern Detection\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **fraud detection** using Semantica with focus on **temporal knowledge graphs**, **anomaly detection**, and **pattern recognition**. The pipeline analyzes transaction streams using temporal knowledge graphs to detect fraud patterns and anomalies in real-time.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Temporal Knowledge Graphs**: Builds temporal KGs to track transaction patterns over time\n",
        "- **Anomaly Detection**: Uses graph-based pattern recognition to identify fraud\n",
        "- **Real-Time Alerts**: Generates automated alerts for detected fraud patterns\n",
        "- **Pattern Recognition**: Identifies suspicious transaction patterns\n",
        "- **Stream Processing**: Demonstrates real-time transaction stream processing\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Transaction Stream Ingestion\n",
        "3. **Phase 2**: Transaction Entity Extraction\n",
        "4. **Phase 3**: Temporal Knowledge Graph Construction\n",
        "5. **Phase 4**: Pattern Detection\n",
        "6. **Phase 5**: Anomaly Detection\n",
        "7. **Phase 6**: Real-Time Alerting\n",
        "8. **Phase 7**: Visualization & Export\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.reasoning import GraphReasoner\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"Fraud_Detection\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\", \"temporal\": True}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "print(\"Configured for fraud detection with temporal KGs focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Real Data Ingestion (Transaction Stream)\n",
        "\n",
        "Ingest transaction data from simulated stream using StreamIngestor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import StreamIngestor, FileIngestor\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Option 1: Ingest from transaction stream (simulated Kafka)\n",
        "# In production: stream_ingestor = StreamIngestor()\n",
        "# stream_documents = stream_ingestor.ingest(\"kafka://localhost:9092/transactions\", method=\"kafka\")\n",
        "\n",
        "# Fallback: Sample transaction stream data\n",
        "tx_data = \"\"\"\n",
        "2024-01-01 10:00:00 - Transaction $1000 from Account A123 to Account B456\n",
        "2024-01-01 10:01:00 - Transaction $5000 from Account A123 to Account C789\n",
        "2024-01-01 10:02:00 - Transaction $10000 from Account A123 to Account D012 (unusual pattern)\n",
        "2024-01-01 10:03:00 - Multiple rapid transactions from Account A123 (suspicious)\n",
        "2024-01-01 10:04:00 - Transaction $2000 from Account B456 to Account E789\n",
        "2024-01-01 10:05:00 - Large transaction $50000 from Account A123 to Account F012 (fraud alert)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"data/transactions.txt\", \"w\") as f:\n",
        "    f.write(tx_data)\n",
        "\n",
        "documents = FileIngestor().ingest(\"data/transactions.txt\")\n",
        "print(f\"Ingested {len(documents)} documents from transaction stream\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 2: Text Normalization & Conflict Detection\n",
        "\n",
        "Normalize transaction data and detect conflicts from multiple sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.conflicts import ConflictDetector\n",
        "\n",
        "# Normalize transaction data\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        normalize_numbers=True,  # Normalize transaction amounts\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Build temporal knowledge graph\n",
        "result = core.build_knowledge_base(\n",
        "    sources=normalized_documents,\n",
        "    custom_entity_types=[\"Transaction\", \"Account\", \"Device\", \"Pattern\", \"Anomaly\"],\n",
        "    graph=True,\n",
        "    temporal=True\n",
        ")\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "entities = result[\"entities\"]\n",
        "\n",
        "# Detect conflicts in transaction data (e.g., conflicting account information)\n",
        "detector = ConflictDetector()\n",
        "conflicts = detector.detect_conflicts(entities, kg.get(\"relationships\", []))\n",
        "\n",
        "print(f\"Built temporal transaction KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"Detected {len(conflicts)} conflicts in transaction data\")\n",
        "if conflicts:\n",
        "    resolved = detector.resolve_conflicts(conflicts, strategy=\"highest_confidence\")\n",
        "    print(f\"Resolved {len(resolved)} conflicts\")\n",
        "print(\"Focus: Temporal KGs, anomaly detection, pattern recognition, real-time alerts\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 3-4: Temporal Pattern Detection\n",
        "\n",
        "Use TemporalPatternDetector for fraud pattern detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import TemporalGraphQuery, TemporalPatternDetector\n",
        "\n",
        "# Initialize temporal pattern detector\n",
        "temporal_query = TemporalGraphQuery(enable_temporal_reasoning=True, temporal_granularity=\"minute\")\n",
        "pattern_detector = TemporalPatternDetector()\n",
        "\n",
        "# Detect temporal fraud patterns\n",
        "fraud_patterns = pattern_detector.detect_patterns(kg, pattern_type=\"fraud\")\n",
        "temporal_patterns = temporal_query.detect_temporal_patterns(kg, pattern_type=\"sequence\")\n",
        "\n",
        "print(f\"Detected {len(fraud_patterns)} fraud patterns\")\n",
        "print(f\"Detected {len(temporal_patterns)} temporal patterns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect fraud patterns using reasoning\n",
        "reasoner = GraphReasoner(kg)\n",
        "reasoning_patterns = reasoner.find_patterns(pattern_type=\"fraud\")\n",
        "\n",
        "# Identify suspicious accounts\n",
        "suspicious_accounts = [e for e in kg.get(\"entities\", []) \n",
        "                       if e.get(\"type\") == \"Account\" and \n",
        "                       any(\"suspicious\" in str(r.get(\"predicate\", \"\")).lower() or\n",
        "                           \"fraud\" in str(r.get(\"predicate\", \"\")).lower()\n",
        "                           for r in kg.get(\"relationships\", []) \n",
        "                           if r.get(\"source\") == e.get(\"id\"))]\n",
        "\n",
        "print(f\"Pattern detection: {len(reasoning_patterns)} fraud patterns from reasoning\")\n",
        "print(f\"Temporal patterns: {len(temporal_patterns)} temporal fraud patterns\")\n",
        "print(f\"Anomaly detection: {len(suspicious_accounts)} suspicious accounts flagged\")\n",
        "print(\"This cookbook emphasizes temporal KGs, TemporalPatternDetector, and pattern-based fraud detection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 7: Visualization & Summary\n",
        "\n",
        "Visualize temporal fraud detection knowledge graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"fraud_detection_kg.html\", layout=\"temporal\")\n",
        "\n",
        "print(\"Fraud detection analysis complete\")\n",
        "print(\"\\n=== Pipeline Summary ===\")\n",
        "print(f\"✓ Ingested {len(documents)} documents from transaction stream\")\n",
        "print(f\"✓ Normalized {len(normalized_documents)} documents\")\n",
        "print(f\"✓ Detected and resolved {len(conflicts)} conflicts\")\n",
        "print(f\"✓ Built temporal KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"✓ Detected {len(reasoning_patterns)} fraud patterns and {len(suspicious_accounts)} suspicious accounts\")\n",
        "print(f\"✓ Emphasizes: Temporal KGs, TemporalPatternDetector, anomaly detection, pattern recognition, real-time alerts\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
