{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Multi-Source Integration\n",
    "\n",
    "This cookbook demonstrates a production-grade workflow for integrating data from distinct sources using Semantica.\n",
    "\n",
    "We will simulate a complex Enterprise Knowledge Graph construction scenario for **Nexus AI** by aggregating data from:\n",
    "\n",
    "1.  **Corporate Database (SQLite)**: Financial records and employee counts.\n",
    "2.  **Public Web (HTML)**: News articles and press releases.\n",
    "3.  **Source Code (Markdown)**: Engineering activity and documentation.\n",
    "4.  **Market Data API (JSON)**: Live stock prices and market cap.\n",
    "5.  **Web Search MCP (Tool)**: Live competitor analysis from a search agent.\n",
    "\n",
    "**Key Semantica Modules Used:**\n",
    "*   `ingest`: For loading data from disparate sources (including MCP).\n",
    "*   `kg.GraphBuilder`: For constructing the graph and merging entities.\n",
    "*   `conflicts.ConflictResolver`: For resolving data discrepancies.\n",
    "*   `visualization.KGVisualizer`: For visualizing the final network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation & Setup\n",
    "!pip install semantica mcp fastmcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "# Import Semantica Modules\n",
    "from semantica.ingest import DBIngestor, WebIngestor, FileIngestor, MCPIngestor\n",
    "from semantica.kg import GraphBuilder\n",
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "# Create a temporary workspace\n",
    "WORKSPACE_DIR = tempfile.mkdtemp()\n",
    "print(f\"Workspace created at: {WORKSPACE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Creating Data Sources\n",
    "\n",
    "We generate files on disk to simulate the disparate enterprise systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SQLite Database (Financials)\n",
    "db_path = os.path.join(WORKSPACE_DIR, \"corporate.db\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "conn.execute(\"CREATE TABLE financials (company_name TEXT, revenue REAL, employees INTEGER)\")\n",
    "conn.execute(\"INSERT INTO financials VALUES ('Nexus AI', 5500000.00, 45)\")\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# 2. Public Web (HTML)\n",
    "html_path = os.path.join(WORKSPACE_DIR, \"news.html\")\n",
    "with open(html_path, \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "    <html><body>\n",
    "        <h1>Nexus AI Raises Series B</h1>\n",
    "        <p>Nexus AI (San Francisco) valuated at $100M.</p>\n",
    "        <p>CEO Jane Doe announces expansion.</p>\n",
    "    </body></html>\n",
    "    \"\"\")\n",
    "\n",
    "# 3. Code Repository (Markdown)\n",
    "repo_path = os.path.join(WORKSPACE_DIR, \"README.md\")\n",
    "with open(repo_path, \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "    # Nexus AI Core\n",
    "    Maintained by: engineering@nexus.ai\n",
    "    Language: Python\n",
    "    \"\"\")\n",
    "\n",
    "# 4. Market Data API (JSON)\n",
    "api_path = os.path.join(WORKSPACE_DIR, \"market.json\")\n",
    "with open(api_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"ticker\": \"NXAI\", \n",
    "        \"price\": 124.50, \n",
    "        \"employees\": 50  # Conflict with DB (45)\n",
    "    }, f)\n",
    "\n",
    "print(\"Data sources created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Ingestion & Extraction\n",
    "\n",
    "We use Semantica's ingestors to load data. In a real pipeline, we would attach an extractor (like an LLM) to parse the raw content into entities. Here, we simulate the extraction output for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Demonstration of Real Ingestion (Raw Data) ---\n",
    "print(\"--- Ingesting Raw Data from Sources (Demonstration) ---\")\n",
    "\n",
    "# 1. Ingest from SQLite using DBIngestor\n",
    "# We connect to the local SQLite database we just created\n",
    "try:\n",
    "    db_ingestor = DBIngestor()\n",
    "    # SQLAlchemy connection string for SQLite\n",
    "    db_connection = f\"sqlite:///{db_path}\"\n",
    "    raw_db_data = db_ingestor.ingest_database(db_connection)\n",
    "    print(f\"DBIngestor: Successfully connected to {db_path}\")\n",
    "    print(f\"DBIngestor: Found tables: {list(raw_db_data.get('tables', {}).keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"DBIngestor Warning: {e}\")\n",
    "\n",
    "# 2. Ingest from Files using FileIngestor\n",
    "# We load the Markdown and JSON files directly\n",
    "try:\n",
    "    file_ingestor = FileIngestor()\n",
    "    \n",
    "    # Ingest Markdown\n",
    "    readme_file = file_ingestor.ingest_file(repo_path)\n",
    "    print(f\"FileIngestor: Read {readme_file.name} ({readme_file.size} bytes)\")\n",
    "    \n",
    "    # Ingest JSON\n",
    "    market_file = file_ingestor.ingest_file(api_path)\n",
    "    print(f\"FileIngestor: Read {market_file.name} ({market_file.size} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"FileIngestor Warning: {e}\")\n",
    "\n",
    "# 3. Ingest from Web using WebIngestor\n",
    "# Since we created a local HTML file, we could use FileIngestor, \n",
    "# but here we demonstrate WebIngestor initialization for URL-based sources.\n",
    "try:\n",
    "    web_ingestor = WebIngestor()\n",
    "    # In a real scenario, we would call: \n",
    "    # web_content = web_ingestor.ingest_url(\"https://nexus.ai/news\")\n",
    "    print(f\"WebIngestor: Initialized and ready to crawl URLs.\")\n",
    "except Exception as e:\n",
    "    print(f\"WebIngestor Warning: {e}\")\n",
    "\n",
    "print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# Simulate extracted entities from our sources\n",
    "\n",
    "source_db = {\n",
    "    \"name\": \"Corporate Database\",\n",
    "    \"type\": \"structured\",\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"name\": \"Nexus AI\",\n",
    "            \"type\": \"Organization\",\n",
    "            \"properties\": {\"revenue\": 5500000.00, \"employees\": 45},\n",
    "            \"source\": \"corporate_db\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "source_web = {\n",
    "    \"name\": \"Web News\",\n",
    "    \"type\": \"unstructured\",\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"name\": \"Nexus AI\",\n",
    "            \"type\": \"Organization\",\n",
    "            \"properties\": {\"valuation\": \"$100M\", \"location\": \"San Francisco\"},\n",
    "            \"source\": \"public_web\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Jane Doe\",\n",
    "            \"type\": \"Person\",\n",
    "            \"properties\": {\"role\": \"CEO\"},\n",
    "            \"source\": \"public_web\"\n",
    "        }\n",
    "    ],\n",
    "    \"relationships\": [\n",
    "        {\"source\": \"Jane Doe\", \"target\": \"Nexus AI\", \"type\": \"is_ceo_of\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "source_api = {\n",
    "    \"name\": \"Market API\",\n",
    "    \"type\": \"structured\",\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"name\": \"Nexus AI\",\n",
    "            \"type\": \"Organization\",\n",
    "            \"properties\": {\"ticker\": \"NXAI\", \"employees\": 50}, # Note conflict: 50 vs 45\n",
    "            \"source\": \"market_api\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "source_repo = {\n",
    "    \"name\": \"GitHub Repo\",\n",
    "    \"type\": \"semi-structured\",\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"name\": \"Nexus AI Core\",\n",
    "            \"type\": \"Software\",\n",
    "            \"properties\": {\"language\": \"Python\"},\n",
    "            \"source\": \"github\"\n",
    "        }\n",
    "    ],\n",
    "    \"relationships\": [\n",
    "        {\"source\": \"Nexus AI Core\", \"target\": \"Nexus AI\", \"type\": \"owned_by\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 5. Web Search MCP\n",
    "# We use Semantica's MCPIngestor to connect to a Web Search MCP server.\n",
    "# This allows us to fetch live competitor data (e.g., from Brave Search).\n",
    "\n",
    "try:\n",
    "    # Initialize MCP Ingestor\n",
    "    mcp = MCPIngestor()\n",
    "    \n",
    "    # Attempt to connect to a local MCP server (e.g., running on port 8000)\n",
    "    # Example: `fastmcp run search_server.py`\n",
    "    mcp.connect(\"web_search\", url=\"http://localhost:8000/sse\")\n",
    "    \n",
    "    print(\"Connected to MCP Server. Ingesting live data...\")\n",
    "    \n",
    "    # Ingest data using the search tool\n",
    "    mcp_data = mcp.ingest_tool_output(\n",
    "        \"web_search\", \n",
    "        \"search\", \n",
    "        {\"query\": \"Nexus AI competitors valuation\"}\n",
    "    )\n",
    "    \n",
    "    # Process the output (assuming the tool returns structured entities)\n",
    "    # In a real app, you might need an LLM to extract entities from search results.\n",
    "    # Here we assume the MCP server returns ready-to-use entities.\n",
    "    source_mcp = {\n",
    "        \"name\": \"Web Search MCP\",\n",
    "        \"type\": \"agent-tool\",\n",
    "        \"entities\": mcp_data.get(\"entities\", []),\n",
    "        \"source\": \"mcp_search\"\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"MCP Server connection failed ({e}). Using simulated data.\")\n",
    "    print(\"To enable live data, ensure an MCP server is running at http://localhost:8000/sse\")\n",
    "    \n",
    "    # Fallback to simulated data\n",
    "    source_mcp = {\n",
    "    \"name\": \"Web Search MCP\",\n",
    "    \"type\": \"agent-tool\",\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"name\": \"Nexus AI\",\n",
    "            \"type\": \"Organization\",\n",
    "            \"properties\": {\n",
    "                \"competitors\": [\"Cyberdyne Systems\", \"Massive Dynamic\"],\n",
    "                \"valuation\": \"$120M\" # Note conflict: $120M (newer) vs $100M (older web)\n",
    "            },\n",
    "            \"source\": \"mcp_search_agent\"\n",
    "        }\n",
    "    ],\n",
    "    \"relationships\": [\n",
    "         {\"source\": \"Nexus AI\", \"target\": \"Cyberdyne Systems\", \"type\": \"competes_with\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "all_sources = [source_db, source_web, source_api, source_repo, source_mcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Graph Construction & Resolution\n",
    "\n",
    "We use `GraphBuilder` to:\n",
    "1.  **Merge Entities**: Combine the 4 \"Nexus AI\" records into one canonical node.\n",
    "2.  **Resolve Conflicts**: Handle discrepancies (Employee count: 45 vs 50; Valuation: $100M vs $120M).\n",
    "3.  **Build Graph**: Link related entities (CEO, Software, Competitors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GraphBuilder with resolution enabled\n",
    "builder = GraphBuilder(\n",
    "    merge_entities=True,\n",
    "    resolve_conflicts=True,\n",
    "    entity_resolution_strategy=\"fuzzy\"\n",
    ")\n",
    "\n",
    "# Build the graph\n",
    "print(\"Building Knowledge Graph...\")\n",
    "kg = builder.build(sources=all_sources)\n",
    "\n",
    "print(f\"Graph built with {len(kg['nodes'])} nodes and {len(kg['edges'])} edges.\")\n",
    "\n",
    "# Verify conflict resolution results\n",
    "nexus_node = next(n for n in kg['nodes'] if n['name'] == \"Nexus AI\")\n",
    "print(\"\\nResolved Properties for Nexus AI:\")\n",
    "print(f\"  Employees: {nexus_node['properties'].get('employees')} (Resolved from DB/API)\")\n",
    "print(f\"  Valuation: {nexus_node['properties'].get('valuation')} (Resolved from Web/MCP)\")\n",
    "print(f\"  Competitors: {nexus_node['properties'].get('competitors')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Visualization\n",
    "\n",
    "We use `KGVisualizer` to render the interactive graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Graph...\")\n",
    "visualizer = KGVisualizer(layout=\"force\", color_scheme=\"vibrant\")\n",
    "\n",
    "# Render the network\n",
    "# This supports interactive output in Jupyter\n",
    "fig = visualizer.visualize_network(kg, output=\"interactive\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we used `semantica`'s high-level modules to:\n",
    "1.  Ingest data from 5 different sources (including a simulated **MCP Search Agent**).\n",
    "2.  Automatically resolve identity to create a single \"Nexus AI\" node.\n",
    "3.  Resolve complex data conflicts (Valuation, Employee Count).\n",
    "4.  Visualize the unified Knowledge Graph with competitor relationships.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}