{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34af0e1d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/advanced/11_Advanced_Context_Engineering.ipynb)\n",
    "\n",
    "# Advanced Context Engineering\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers advanced topics in context engineering using Semantica. We will explore custom memory management strategies, tuning hybrid retrieval, and extending the system with custom graph builders.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- **Custom Memory Pruning**: Implement importance-based pruning instead of standard FIFO/token-based pruning.\n",
    "- **Custom Graph Extensions**: Register custom graph building methods using the registry system.\n",
    "- **Hybrid Retrieval Tuning**: Optimize weights for vector and graph search.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We'll start by setting up the environment and initializing a standard Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semantica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbd8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from semantica.context import AgentMemory, AgentContext, ContextGraph, ContextRetriever\n",
    "from semantica.vector_store import VectorStore\n",
    "from semantica.context import registry, methods\n",
    "\n",
    "# Configure logging to see internal processes\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize Vector Store (using in-memory backend for this example)\n",
    "# In production, you might use 'weaviate', 'qdrant', or 'faiss'\n",
    "vs = VectorStore(backend=\"inmemory\", dimension=384)\n",
    "\n",
    "# Initialize Context Graph\n",
    "kg = ContextGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8299a",
   "metadata": {},
   "source": [
    "## 2. Custom Memory Pruning Strategy\n",
    "\n",
    "By default, `AgentMemory` uses a FIFO (First-In-First-Out) strategy combined with a token limit to prune short-term memory. However, you might want to keep \"important\" memories longer regardless of their age.\n",
    "\n",
    "Let's subclass `AgentMemory` to implement an importance-based pruning strategy that respects metadata flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportanceAwareMemory(AgentMemory):\n",
    "    def _prune_short_term_memory(self) -> None:\n",
    "        \"\"\"\n",
    "        Custom pruning: Always keep items marked as 'important' in metadata,\n",
    "        then prune others based on token limits.\n",
    "        \"\"\"\n",
    "        if not self.short_term_memory:\n",
    "            return\n",
    "\n",
    "        # Separate important items\n",
    "        important_items = [item for item in self.short_term_memory if item.metadata.get(\"important\")]\n",
    "        other_items = [item for item in self.short_term_memory if not item.metadata.get(\"important\")]\n",
    "        \n",
    "        # Calculate tokens used by important items\n",
    "        important_tokens = sum(self._count_tokens(item.content) for item in important_items)\n",
    "        \n",
    "        # Calculate remaining budget\n",
    "        remaining_tokens = max(0, self.token_limit - important_tokens)\n",
    "        \n",
    "        # Prune other items to fit remaining budget\n",
    "        kept_others = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        # Iterate in reverse (newest first) to keep recent items\n",
    "        for item in reversed(other_items):\n",
    "            item_tokens = self._count_tokens(item.content)\n",
    "            if current_tokens + item_tokens <= remaining_tokens:\n",
    "                kept_others.insert(0, item)\n",
    "                current_tokens += item_tokens\n",
    "            else:\n",
    "                break # Stop once we hit the limit\n",
    "                \n",
    "        # Reconstruct memory: Important items + kept recent items\n",
    "        # Sort by timestamp to maintain order\n",
    "        all_kept = sorted(important_items + kept_others, key=lambda x: x.timestamp)\n",
    "        self.short_term_memory = all_kept\n",
    "\n",
    "# Initialize our custom memory with a strict token limit for testing\n",
    "memory = ImportanceAwareMemory(\n",
    "    vector_store=vs, \n",
    "    token_limit=100, \n",
    "    short_term_limit=50\n",
    ")\n",
    "\n",
    "# 1. Store an OLD but IMPORTANT memory\n",
    "memory.store(\"IMPORTANT: User's name is Alice\", metadata={\"important\": True})\n",
    "\n",
    "# 2. Flood memory with newer filler content\n",
    "for i in range(20):\n",
    "    memory.store(f\"Filler memory {i} \" * 5) # This consumes tokens\n",
    "\n",
    "print(f\"Short-term items count: {len(memory.short_term_memory)}\")\n",
    "print(\"First item (should be the important one):\", memory.short_term_memory[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f857653",
   "metadata": {},
   "source": [
    "## 3. Extending with Custom Graph Methods\n",
    "\n",
    "Semantica's registry system allows you to plug in custom logic for graph construction, retrieval, and more. This is powerful for domain-specific graph topologies.\n",
    "\n",
    "Let's register a custom graph builder that creates a \"Star Graph\" topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def star_graph_builder(\n",
    "    entities: Optional[List[Dict[str, Any]]] = None,\n",
    "    relationships: Optional[List[Dict[str, Any]]] = None,\n",
    "    conversations: Optional[List[Any]] = None,\n",
    "    center_entity: str = \"Central Hub\",\n",
    "    satellites: Optional[List[str]] = None,\n",
    "    **kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Builds a star graph where all satellites connect to the center.\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    \n",
    "    # Center node\n",
    "    nodes.append({\"id\": \"center\", \"type\": \"CENTER\", \"properties\": {\"content\": center_entity}})\n",
    "    \n",
    "    satellites = satellites or []\n",
    "    for i, sat in enumerate(satellites):\n",
    "        sat_id = f\"sat_{i}\"\n",
    "        nodes.append({\"id\": sat_id, \"type\": \"SATELLITE\", \"properties\": {\"content\": sat}})\n",
    "        edges.append({\"source_id\": \"center\", \"target_id\": sat_id, \"type\": \"connects_to\"})\n",
    "        \n",
    "    return {\n",
    "        \"nodes\": nodes, \n",
    "        \"edges\": edges, \n",
    "        \"statistics\": {\"node_count\": len(nodes), \"edge_count\": len(edges)}\n",
    "    }\n",
    "\n",
    "# Register the method in the global registry\n",
    "registry.method_registry.register(\"graph\", \"star_builder\", star_graph_builder)\n",
    "\n",
    "# Verify registration\n",
    "print(\"Available graph methods:\", registry.method_registry.list_all(\"graph\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c3c76",
   "metadata": {},
   "source": [
    "Now we can use this method via the standard `methods` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a324b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a graph using our custom method\n",
    "graph_data = methods.build_context_graph(\n",
    "    method=\"star_builder\",\n",
    "    center_entity=\"Solar System\",\n",
    "    satellites=[\"Earth\", \"Mars\", \"Jupiter\", \"Venus\"]\n",
    ")\n",
    "\n",
    "print(f\"Created graph with {len(graph_data['nodes'])} nodes and {len(graph_data['edges'])} edges.\")\n",
    "print(\"Edges sample:\", graph_data['edges'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b2093",
   "metadata": {},
   "source": [
    "## 4. Tuning Hybrid Retrieval\n",
    "\n",
    "Hybrid retrieval combines scores from vector search and graph traversal. You can tune the `hybrid_alpha` parameter to weight these components.\n",
    "\n",
    "- `hybrid_alpha = 0.0`: Pure Vector Search\n",
    "- `hybrid_alpha = 1.0`: Pure Graph Search\n",
    "- `hybrid_alpha = 0.5`: Balanced (Default)\n",
    "\n",
    "Let's configure a `ContextRetriever` with a preference for graph connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate knowledge graph with some test data\n",
    "kg.add_node(\"python\", \"concept\", \"Python\")\n",
    "kg.add_node(\"ml\", \"concept\", \"Machine Learning\")\n",
    "kg.add_edge(\"python\", \"ml\", \"used_for\")\n",
    "\n",
    "# Initialize retriever with custom tuning\n",
    "retriever = ContextRetriever(\n",
    "    memory_store=memory,\n",
    "    knowledge_graph=kg,\n",
    "    vector_store=vs,\n",
    "    hybrid_alpha=0.7,      # Favor graph connections\n",
    "    max_expansion_hops=2   # Traverse deeper in the graph\n",
    ")\n",
    "\n",
    "# Retrieve\n",
    "results = retriever.retrieve(\"Python\")\n",
    "\n",
    "print(f\"Found {len(results)} results.\")\n",
    "for res in results:\n",
    "    print(f\"Source: {res.source}, Score: {res.score:.2f}, Content: {res.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc770d0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully extended Semantica's context capabilities by:\n",
    "1.  Implementing a custom memory pruning logic.\n",
    "2.  Registering a new graph construction algorithm.\n",
    "3.  Tuning the hybrid retrieval parameters.\n",
    "\n",
    "These patterns allow you to adapt the context engine to specialized domain requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
